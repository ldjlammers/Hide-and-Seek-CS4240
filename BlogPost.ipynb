{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BlogPost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPd1Qb2cWqM5qAsNoaJq66K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldjlammers/Hide-and-Seek-CS4240/blob/master/BlogPost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vsvG125OuiR",
        "colab_type": "text"
      },
      "source": [
        "![alt text](./in-text-images/HideandSeekVisual.jpg)\n",
        "\n",
        "# The Hide and Seek method for classifaction and localisation\n",
        "\n",
        "*By: Laurens Lammers and Mink van Oosterhout*\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "In this blog post we try to reproduce a method called \"Hide and Seek\", proposed by Singh and Lee in their 2017 paper '[Hide-and-Seek](https://arxiv.org/abs/1704.04232): Forcing a network to be Meticulous for weakly-supervised Object and Action Localization'. In particular we focus on object localization in images.\n",
        "\n",
        "This blog post is ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYE7wWHtXl5z",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. What is Hide and Seek?\n",
        "\n",
        "'Hide and Seek' is a weakly-supervised framework that intents to improve object localization in images and action localization in videos. Instead of making algoritmic changes, or relying on external data, the Hide and Seek method makes changes to the input images. The key idea is to hide patches of the training images rondomly, forcing the network to seek other relevant parts when the most discriminative part is hidden. This principle is visualised in the image above at the top of the blog. The advantage of such an approach is that it can be applied to any network which is designed for object localisation. \n",
        "\n",
        "Most weakly-supervised localization methods identify discriminative patterns in the training data. These disciminative patterns are usually areas which occor frequently in one class and hardly ever in other classes. Due to variations within classes and leaning to much on just classification, these approaches frequently do not succeed in identifying the whole extent of an object. Instead, they only localize the most discriminative part of the object. Two examples of this can be seen in the image below. Because the head of the rabbit and the cilinder of the revolver distinguish them the most relative to other classes, the classiefer (over)focusses on these regions.\n",
        "\n",
        "![alt text](./in-text-images/discriminativeparts.jpg)\n",
        "\n",
        "To adress this problem, [Zhou et al.](https://arxiv.org/pdf/1512.04150.pdf) (2016) replaced max pooling after the final convolutional layer in a classification network with global average pooling. In this configuration, a very small maximum can no longer dictate the activation of an entire feature maps. Instead, it forces the network to look beyond the most discriminative parts, in order to achieve activation for a certain feature map. However, this approach did not solve the problem entirely. The network can still avoid learing less discriminative parts by finding more than one high-discriminative ones\n",
        "\n",
        "The authors of the Hide and Seek paper take a 'radically different' approach. Instead of making changes to a network architecture or (hyper)parameters, they modify the input image. By *hiding* patches of the input image, they force the network to *seek* less discriminative regions. Hence, they propose the 'Hide and Seek' method. By hiding random patches, it may happen that the most discriminative part of an image (e.g. face or characteristic shape) is invisible to the model. By hiding different patches in each training epoch, the model sees different parts of an image each epoch and will thus have to focus on multiple discriminative regions, instead of just one.\n",
        "\n",
        "Finally, an important aspect of the Hide and Seek method is the fact that no patches are hidden during validation. Consequently, the distribution of the training data will be dissimilar to the distribution of the test data. But how how will such a model generalize then? We will find out in the next section..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXcWMGrnZ4A0",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. The paper's approach\n",
        "\n",
        "In this section the Hide and Seek method is described in more detail. We start by laying out the general framework of the approach. After this, the specific aspects are explained one by one.\n",
        "\n",
        "\n",
        "### Weakly-supervised object localization\n",
        "\n",
        "Given a set of images $I_{set} = \\{ I_1, I_2, ..., I_N\\}$, the goal of weakly-supervised object localisation is to learn an object localizer that can predict both the category label, as well as the bounding box for that same object, in a new test image. This is achiebed by training a convolutional neural network with just the image-level labels. That is, no ground truth bounding boxes are used during training. \n",
        "\n",
        "![alt text](./in-text-images/approach.jpg)\n",
        "\n",
        "\n",
        "### Hiding random patches\n",
        "The objective of hiding patches of the training images is to display many parts of the image to the network, while simultaneously training it for the classification task. By hiding the patches randomly, the network is forced to explore the less discriminative parts of the image, immproving the localization abilities. \n",
        "\n",
        "Given an input image $I$ of size $W x H x 3$, it is divided into a grid with fixed patch size of $S x S x 3$. Subsequently, each individual patch is hidden with probability $p_{hide} = 0.5$. This process is shown in the left part of the figure above. The new image $I'$ is fed into the CNN for classification. In addition, for each image, a different random set of patches is hidden and also, for the same image, a new random set of patches is hidden every epoch.\n",
        "\n",
        "During testing, the entire images is given as input to the network. This can be seen in the right part of the image above. Since the network has already learned to focus on multiple parts of the image, it is not necessary to hide patches. \n",
        "\n",
        "\n",
        "### Setting hidden pixel values\n",
        "By hiding certain patches of the training images, a difference arises between the training and testing data distributions. For a trained network to generalize well to the test data, the distributions of the first conv layer activations should be approximately equal. Howerver, some patches of the training images will be hidden while the test images will always be original with no hidden patches. \n",
        "\n",
        "Given a convolution filter $F$ with kernel size $K x K$ and weights $W = \\{w_1, w_2, ..., w_{kxk}\\}$, which is applied to an RGB patch $X = \\{x_1, x_2, ..., x_{kxk}\\}$ in input image $I'$. For all units in the network that are connected to $x$ units with $w$ outgoing weights, the distribution of $w^\\top x$ has to be rougly equal during training and testing. The paper distinguishes three cases:\n",
        "\n",
        "*   F is completely in visible patch &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp;&emsp;&emsp;&emsp; <font color = blue> $\\sum_{i=1}^{k x k} w_{i}^{\\top} x_i$ </font>\n",
        "\n",
        "*   F is completely in hidden patch &emsp; &emsp; &emsp; &emsp; &emsp;&emsp;&emsp;&emsp; &emsp; &emsp; &emsp; <font color = red> $\\sum_{i=1}^{k x k} w_{i}^{\\top} v$ </font>\n",
        "\n",
        "*   F is partially in hidden patch &emsp; &emsp;&emsp;&emsp; &emsp; &emsp; &emsp; <font color = green> $\\sum_{m \\in visible} w_{m}^{\\top} x_m + \\sum_{n \\in hidden} w_{n}^{\\top} v$  </font>\n",
        "\n",
        "were v is the RGB value. The color codes correspond to the examplee image below\n",
        "\n",
        "![alt text](./in-text-images/hiddenpixels.jpg)\n",
        "\n",
        "Only when F is completely within a visible patch, the output will be equal to a test image. For the other cases, the activations will have a distribution that is different to those seen in training.\n",
        "\n",
        "The solution to this problem is setting the RGB value $v$ of the hidden pixels to be equal to the mean RGB vector of the imagees overe the entire dataset:\n",
        "\n",
        "\n",
        "\n",
        "### Architectures\n",
        "\n",
        "The AlexNet architechture was first proposed by *Krizhevsky et al.* in their [original paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) called \"ImageNet Classification with Deep Convolutional\n",
        "Neural Networks\". The network was created for image classification as an entry to the ImageNet LSVRC-2010 contest.  The Hide and Seek paper, which is reproduced in this notebook, does make a few changes to the original AlexNet architecture. The layers after conv5 (i.e., pool5 to prob) resulting in a mapping resolution of 13 Ã— 13, are all removed. \n",
        "\n",
        "![alt text](./in-text-images/AlexNetScheme.jpg)\n",
        "\n",
        "Instead, after conv5, a convolutional layer of size 3 Ã— 3, stride 1, pad 1 with 1024 units, followed by a GAP layer and a softmax layer, are added. \n",
        "\n",
        "The weights are initialized in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. The neuron biases are initialized in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1.\n",
        "\n",
        "![alt text](./in-text-images/GoogLeNetScheme.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW7aiWz1_sjM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 4. Results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwHSNKrKctow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BlogPost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldjlammers/Hide-and-Seek-CS4240/blob/master/BlogPost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vsvG125OuiR",
        "colab_type": "text"
      },
      "source": [
        "![alt text](./in-text-images/HideandSeekVisual.jpg)\n",
        "\n",
        "# The Hide and Seek method for classifaction and localisation\n",
        "\n",
        "*By: Laurens Lammers and Mink van Oosterhout*\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "In this blog post we try to reproduce a method called \"Hide and Seek\", proposed by Singh and Lee in their 2017 paper '[Hide-and-Seek](https://arxiv.org/abs/1704.04232): Forcing a network to be Meticulous for weakly-supervised Object and Action Localization'. In particular we focus on object localization in images.\n",
        "\n",
        "In the second section we explain what Hide and Seek approach generally is about. In the third section we analyse the detailed aspects of the Hide and Seek approach. After this, in the fourth section we discuss the results. Lastly, the fifth section we write our conclusions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYE7wWHtXl5z",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. What is Hide and Seek?\n",
        "\n",
        "'Hide and Seek' is a weakly-supervised framework that intents to improve object localization in images and action localization in videos. Instead of making algoritmic changes, or relying on external data, the Hide and Seek method makes changes to the input images. The key idea is to hide patches of the training images rondomly, forcing the network to seek other relevant parts when the most discriminative part is hidden. This principle is visualised in the image above at the top of the blog. The advantage of such an approach is that it can be applied to any network which is designed for object localisation. \n",
        "\n",
        "Most weakly-supervised localization methods identify discriminative patterns in the training data. These disciminative patterns are usually areas which occor frequently in one class and hardly ever in other classes. Due to variations within classes and leaning to much on just classification, these approaches frequently do not succeed in identifying the whole extent of an object. Instead, they only localize the most discriminative part of the object. Two examples of this can be seen in the image below. Because the head of the rabbit and the cilinder of the revolver distinguish them the most relative to other classes, the classiefer (over)focusses on these regions.\n",
        "\n",
        "![alt text](./in-text-images/discriminativeparts.jpg)\n",
        "\n",
        "To adress this problem, [Zhou et al.](https://arxiv.org/pdf/1512.04150.pdf) (2016) replaced max pooling after the final convolutional layer in a classification network with global average pooling. In this configuration, a very small maximum can no longer dictate the activation of an entire feature maps. Instead, it forces the network to look beyond the most discriminative parts, in order to achieve activation for a certain feature map. However, this approach did not solve the problem entirely. The network can still avoid learing less discriminative parts by finding more than one high-discriminative ones\n",
        "\n",
        "The authors of the Hide and Seek paper take a 'radically different' approach. Instead of making changes to a network architecture or (hyper)parameters, they modify the input image. By *hiding* patches of the input image, they force the network to *seek* less discriminative regions. Hence, they propose the 'Hide and Seek' method. By hiding random patches, it may happen that the most discriminative part of an image (e.g. face or characteristic shape) is invisible to the model. By hiding different patches in each training epoch, the model sees different parts of an image each epoch and will thus have to focus on multiple discriminative regions, instead of just one.\n",
        "\n",
        "Finally, an important aspect of the Hide and Seek method is the fact that no patches are hidden during validation. Consequently, the distribution of the training data will be dissimilar to the distribution of the test data. But how how will such a model generalize then? We will find out in the next section..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXcWMGrnZ4A0",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. The paper's approach\n",
        "\n",
        "In this section the Hide and Seek method is described in more detail. We start by laying out the general framework of the approach. After this, the specific aspects are explained one by one.\n",
        "\n",
        "\n",
        "### 3.1 Weakly-supervised object localization\n",
        "\n",
        "Given a set of images $I_{set} = \\{ I_1, I_2, ..., I_N\\}$, the goal of weakly-supervised object localisation is to learn an object localizer that can predict both the category label, as well as the bounding box for that same object, in a new test image. This is achiebed by training a convolutional neural network with just the image-level labels. That is, no ground truth bounding boxes are used during training. \n",
        "\n",
        "![alt text](./in-text-images/approach.jpg)\n",
        "\n",
        "\n",
        "### 3.2 Hiding random patches\n",
        "The objective of hiding patches of the training images is to display many parts of the image to the network, while simultaneously training it for the classification task. By hiding the patches randomly, the network is forced to explore the less discriminative parts of the image, immproving the localization abilities. \n",
        "\n",
        "Given an input image $I$ of size $W x H x 3$, it is divided into a grid with fixed patch size of $S x S x 3$. Subsequently, each individual patch is hidden with probability $p_{hide} = 0.5$. This process is shown in the left part of the figure above. The new image $I'$ is fed into the CNN for classification. In addition, for each image, a different random set of patches is hidden and also, for the same image, a new random set of patches is hidden every epoch.\n",
        "\n",
        "During testing, the entire images is given as input to the network. This can be seen in the right part of the image above. Since the network has already learned to focus on multiple parts of the image, it is not necessary to hide patches. \n",
        "\n",
        "\n",
        "### 3.3 Setting hidden pixel values\n",
        "By hiding certain patches of the training images, a difference arises between the training and testing data distributions. For a trained network to generalize well to the test data, the distributions of the first conv layer activations should be approximately equal. Howerver, some patches of the training images will be hidden while the test images will always be original with no hidden patches. \n",
        "\n",
        "Given a convolution filter $F$ with kernel size $K x K$ and weights $W = \\{w_1, w_2, ..., w_{kxk}\\}$, which is applied to an RGB patch $X = \\{x_1, x_2, ..., x_{kxk}\\}$ in input image $I'$. For all units in the network that are connected to $x$ units with $w$ outgoing weights, the distribution of $w^\\top x$ has to be rougly equal during training and testing. The paper distinguishes three cases:\n",
        "\n",
        "*   (Red) F is completely in visible patch, with output: &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp;&emsp;&emsp;&emsp; $\\sum_{i=1}^{k x k} w_{i}^{\\top} x_i$ \n",
        "\n",
        "*   (Blue) F is completely in hidden patch, with output: &emsp; &emsp; &emsp; &emsp; &emsp;&emsp;&emsp;&emsp; &emsp; &emsp; &emsp; $\\sum_{i=1}^{k x k} w_{i}^{\\top} v$ \n",
        "\n",
        "*   (Green) F is partially in hidden patch, with output &emsp; &emsp;&emsp;&emsp; &emsp; &emsp; &emsp; $\\sum_{m \\in visible} w_{m}^{\\top} x_m + \\sum_{n \\in hidden} w_{n}^{\\top} v$  \n",
        "\n",
        "were v is the RGB value. The color codes correspond to the examplee image below.\n",
        "\n",
        "![alt text](./in-text-images/hiddenpixels.jpg)\n",
        "\n",
        "Only when F is completely within a visible patch, the output will be equal to that a test image. For the other cases, the activations will have a distribution that is different to those seen in training.\n",
        "\n",
        "The solution to this problem is setting the RGB value $v$ of the hidden pixels equal to the mean RGB vector of the imagees overe the entire dataset:\n",
        "\n",
        "$$v = \\mu = \\frac{1}{N_{pixels}} \\sum_{j} x_j$$\n",
        "\n",
        "where j indexes over all the pixels in the dataset and $N_{pixels}$ is the total number of pixels in the dataset. \n",
        "\n",
        "\n",
        "### 3.4 Architectures\n",
        "\n",
        "The Hide and Seek paper utilizes two different CNN architectures: AlexNet and GoogLeNet.\n",
        "\n",
        "#### 3.4.1 AlexNet\n",
        "The AlexNet architechture was first proposed by *Krizhevsky et al.* in their [original paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) called \"ImageNet Classification with Deep Convolutional\n",
        "Neural Networks\". The network was created for image classification as an entry to the ImageNet LSVRC-2010 contest.  The Hide and Seek paper does make a few changes to the original AlexNet architecture. The layers after conv5 (i.e., pool5 to prob) resulting in a mapping resolution of 13 × 13, are all removed. Instead, after conv5, two convolutional layers are added. One convolutional layer of size 3 × 3, stride 1, pad 1 with 512 units and one convolutional layer of size 3 × 3, stride 1, pad 1 with 1024 units. These added conv layers are followed by a GAP layer and a softmax layer. The architecture is shown schematically in the figure below.  \n",
        "\n",
        "![alt text](./in-text-images/AlexNetScheme.jpg)\n",
        "\n",
        "#### 3.4.2 GoogLeNet\n",
        "\n",
        "The AlexNet architechture was first proposed by *Szegedy et al.* in their [original paper](https://static.googleusercontent.com/media/research.google.com/nl//pubs/archive/43022.pdf) called \"Going deeper with convolution\". The network was created for image classification as an entry to the ImageNet LSVRC-2014 contest. The modification made to GoogLeNet differ slightly with respect to the changes made to the AlexNet architecture. For GoogLeNet, we removed the layers after inception4e (i.e., pool4 to prob), resulting in a mapping resolution of 14 × 14, but then added just one convolutional layer of size 3 × 3, stride 1, pad 1 with 1024 units. This added conv layer is followed by a GAP layer and a softmax layer. The architecture is shown schematically in the figure below.\n",
        "\n",
        "![alt text](./in-text-images/GoogLeNetScheme.png)\n",
        "\n",
        "### 3.5 Class activation maps\n",
        "\n",
        "To generate a CAM for an image, global average pooling is performed after the last convolutional layer and the result is given to a classification layer to predict the image’s class probabilities. The weights associated with a class in\n",
        "the classification layer represent the importance of the last convolutional layer’s feature maps for that class.\n",
        "\n",
        "Given the CAM for an image, it is possible to generate a bounding box. By thresholding the CAM to 20% and 30% of the max value for AlexNet and GoogleNet respectiveely, you produce a binary foreground/background map, and then find connected components among the foreground pixels. Finally, it is possibl to fit a tight bounding box to the largest connected component. \n",
        "\n",
        "![alt text](./in-text-images/Cams.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW7aiWz1_sjM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 4. Results\n",
        "\n",
        "For the reproduction of the Hide and Seek paper, we did not use thee original ILSVRC dataset. Instead, we used alternative ImageNet data. Namely, 50 classes of 1050 images each. The data was split 90/10 into training and test data, respectively. \n",
        "\n",
        "### Object localization quantitative results\n",
        "\n",
        "The localization accuracy on ILSVRC validatin data of the Hide and Seek paper.\n",
        "\n",
        "| Method || Top-1 Loc | Top-1 Clas |\n",
        "|------||------|------|\n",
        "|   AlexNet-GAP || 36.25 | 60.23 |\n",
        "|   AlexNet-HaS || 37.65 | 58.68 |\n",
        "| || | | \n",
        "| GoogLeNet-GAP ||  43.60 | 71.95 |\n",
        "| GoogLeNet-HaS ||  - | - |\n",
        "\n",
        "The localization accuracy on ILSVRC validatin data of the Hide and Seek paper.\n",
        "\n",
        "| Method || Top-1 Loc | Top-1 Clas |\n",
        "|------||------|------|\n",
        "|   AlexNet-GAP || 34,95 | 66,10 |\n",
        "|   AlexNet-HaS || 23,23 | 43,7 |\n",
        "| || | | \n",
        "| GoogLeNet-GAP ||  - | 87,08 |\n",
        "| GoogLeNet-HaS ||  - | 83,14 |\n",
        "\n",
        "The localization accuracy on Image validatin data of our model.\n",
        "\n",
        "\n",
        "\n",
        "### Oject localization qualitative results\n",
        "Displayed below at the very bottom is the class activation map for a test image with ground truth class 'monarch'. AlexNet correctly predicted the class, and the heat map intuitively shows the areas of interest. However, the bounding boxes predicted by the algorithm are too broad. The predicted bounding boxes are very sensitive to the treshold value which is used in the segmentation of the CAM, as is demonstrated by the impact of varying treshold values. From left to right, the left image has the treshold value of 0.8, the next image 0.4, and the most left image 0.3 (the value in the original paper). With lower tresholds, the bounding box is likely to be taken too big, whereas for higher tresholds, the final bounding boxes are very sensitive to local maxima. \n",
        "Also displayed below are CAMs for a pomegrenate ground truth class image, from left to right, the CAM is displayed for: pomegrenate, pumpkin, breakfast.\n",
        "![alt text](./in-text-images/vlinder.jpg)\n",
        "\n",
        "![alt text](./in-text-images/mango.jpg)\n",
        "\n",
        "![alt text](./in-text-images/CAMvlinder.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn_XuGJ4htmt",
        "colab_type": "text"
      },
      "source": [
        "Displayed below is the class activation map for a test image with ground truth class 'monarch'. The model correctly predicted the class, and the heat map intuitively shows the areas of interest. However, the bounding boxes predicted by the algorithm are too broad. The predicted bounding boxes are very sensitive to the treshold value which is used in the segmentation of the CAM, as is demonstr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbtqdBVafrju",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 5. Conclusion\n",
        "\n",
        "Due to several factors, we were not able to reproducee the results of the Hide and Seek paper. This is not because the Hide and Seek paper is a poor written paper, but because we did not use the same data set and struggled massively in running on Google Colab, because it kept disconnecting while running. Then the GPU time was up and we had to wait 12 hours. After 5 days of trying on Google Colab without succes we managed to switch to a Google Cloud deep learing VM very last minute. In this very small amount of time we managed to gather some results, but unfortunateely not enough to reproduce the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgjaUJGMm1sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
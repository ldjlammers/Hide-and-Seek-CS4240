{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hide-and-Seek_Reproducibility_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldjlammers/Hide-and-Seek-CS4240/blob/master/Hide_and_Seek_Reproducibility_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQPiEX815rQh",
        "colab_type": "text"
      },
      "source": [
        "# Reproducing Hide and Seek method for Classification and Localisation \n",
        "\n",
        "In this notebook we try to reproduce a method called \"Hide and Seek\", proposed by Singh and Lee in their 2017 paper '[Hide-and-Seek](https://arxiv.org/abs/1704.04232): Forcing a network to be Meticulous for weakly-supervised Object and Action Localization'. Blah blah blah \n",
        "\n",
        "---\n",
        "\n",
        "# What is Hide and Seek?\n",
        "\n",
        "Instead of making algoritmic changes, or relying on external data, the Hide and Seek method makes changes to the input image. The key idea is to hide patches of the training images rondomly, forcing the network to seek other relevant parts when the most discriminative part is hidden. \n",
        "\n",
        "In order to check wether the network has resorted to other areas of a photo then the most discriminative parts to \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6TKp7b3Y5ZE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Running the reproducibility project in Google Colab\n",
        "\n",
        "In order to run the experiment in Google Colab we had to mount the drive in google drive. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zszys5_3-gv8",
        "colab_type": "code",
        "outputId": "58d0fc0e-b808-47d5-ad2b-4857261615da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "# Mounting the Google drive \n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Specifying the path to the  project folder directory\n",
        "!ls '/content/gdrive/My Drive/Hide and Seek' \n",
        "root_path = 'gdrive/My Drive/Hide and Seek' \n",
        "path ='/content/gdrive/My Drive/Hide and Seek'\n",
        "os.chdir(path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            " 3.jpg\t\t\t   'HaS temp.ipynb'\n",
            " 3.png\t\t\t    Hide-and-Seek_Reproducibility_Project.ipynb\n",
            " AlexNetScheme.png\t    ImgswithBB.xlsx\n",
            " Annotation.tar.gz\t    INDL\n",
            " data\t\t\t    pretrained_models\n",
            " final_synids.xlsx\t    Reproducibility_Setup.ipynb\n",
            "'googlenet measurement 1'   save_trained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q77Yo8v9ZPdZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Architectures: AlexNet and GoogLeNet\n",
        "\n",
        "The AlexNet architechture was first proposed by *Krizhevsky et al.* in their [original paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) called \"ImageNet Classification with Deep Convolutional\n",
        "Neural Networks\". The network was created for image classification as an entry to the ImageNet LSVRC-2010 contest.  The Hide and Seek paper, which is reproduced in this notebook, does make a few changes to the original AlexNet architecture. The layers after conv5 (i.e., pool5 to prob) resulting in a mapping resolution of 13 × 13, are all removed. \n",
        "\n",
        "![AlexNet Schematic](https://drive.google.com/uc?export=view&id=1Rj-GGuBwexK39x3fKgBwOKYwUHXcgvdO)\n",
        "\n",
        "Instead, after conv5, a convolutional layer of size 3 × 3, stride 1, pad 1 with 1024 units, followed by a GAP layer and a softmax layer, are added. \n",
        "\n",
        "The weights are initialized in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. The neuron biases are initialized in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNgDZ7zuaxFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from collections import OrderedDict\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=50, init_weights=True):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv1', nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0)),\n",
        "            ('Relu1', nn.ReLU(inplace=True)),\n",
        "            ('maxpool1', nn.MaxPool2d(kernel_size=3, stride=2)),\n",
        "            ('conv2', nn.Conv2d(96, 256, kernel_size=5, padding=2)),\n",
        "            ('Relu2', nn.ReLU(inplace=True)),\n",
        "            ('maxpool2', nn.MaxPool2d(kernel_size=3, stride=2)),\n",
        "            ('conv3', nn.Conv2d(256, 384, kernel_size=3, padding=1)),\n",
        "            ('Relu3', nn.ReLU(inplace=True)),\n",
        "            ('conv4', nn.Conv2d(384, 384, kernel_size=3, padding=1)),  \n",
        "            ('Relu4', nn.ReLU(inplace=True)),\n",
        "            ('conv5', nn.Conv2d(384, 256, kernel_size=3, padding=1)), \n",
        "            ('Relu4', nn.ReLU(inplace=True)),\n",
        "        ]))\n",
        "\n",
        "        self.extraconvs = nn.Sequential(OrderedDict([\n",
        "          ('extraconv1', nn.Conv2d(256, 512, kernel_size=3, padding=1)),                                           \n",
        "          ('extraconv2', nn.Conv2d(256, 1024, kernel_size=3, padding=1))\n",
        "        ]))\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "        self.classifier = nn.Sequential(OrderedDict([\n",
        "            ('extrafc', nn.Linear(1024 * 1 * 1, num_classes)),\n",
        "            ('softmax', nn.Softmax(dim=1))                                       \n",
        "        ]))\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        bias_conv = ['conv2', 'conv4', 'conv5']\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                init.normal_(module.weight, std=1e-2)\n",
        "                if name in bias_conv:\n",
        "                    init.constant_(module.bias, 1)\n",
        "            elif isinstance(module, nn.Linear):\n",
        "                init.normal_(module.weight, std=1e-2)\n",
        "                init.constant_(module.bias, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.extraconvs(x)\n",
        "        feat_maps = x\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, feat_maps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPcS98Bp7-fT",
        "colab_type": "text"
      },
      "source": [
        "For GoogLeNet, we removed the layers after inception4e (i.e., pool4 to prob), resulting in a mapping resolution of 14 × 14. \n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1arovt362S3Ud8mcVOP6ZM-Ws2suPZj2E)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEpzKgzTGs0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from __future__ import division\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional, Tuple\n",
        "from torch import Tensor\n",
        "\n",
        "class Inception(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj,\n",
        "                 conv_block=None):\n",
        "        super(Inception, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            conv_block(in_channels, ch3x3red, kernel_size=1),\n",
        "            conv_block(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            conv_block(in_channels, ch5x5red, kernel_size=1),\n",
        "            # Here, kernel_size=3 instead of kernel_size=5 is a known bug.\n",
        "            # Please see https://github.com/pytorch/vision/issues/906 for details.\n",
        "            conv_block(ch5x5red, ch5x5, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n",
        "            conv_block(in_channels, pool_proj, kernel_size=1)\n",
        "        )\n",
        "        \n",
        "\n",
        "    def _forward(self, x):\n",
        "        branch1 = self.branch1(x)\n",
        "        branch2 = self.branch2(x)\n",
        "        branch3 = self.branch3(x)\n",
        "        branch4 = self.branch4(x)\n",
        "\n",
        "        outputs = [branch1, branch2, branch3, branch4]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionAux(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, num_classes, conv_block=None):\n",
        "        super(InceptionAux, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(2048, 1024)\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):       \n",
        "        x = F.adaptive_avg_pool2d(x, (4, 4))  # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14       \n",
        "        x = self.conv(x)                      # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4      \n",
        "        x = torch.flatten(x, 1)               # N x 128 x 4 x 4        \n",
        "        x = F.relu(self.fc1(x), inplace=True) # N x 2048        \n",
        "        x = F.dropout(x, 0.7, training=self.training)   # N x 1024        \n",
        "        x = self.fc2(x)                       # N x 1024\n",
        "        x = self.softmax(x)\n",
        "        return x                              # N x 1000 (num_classes)\n",
        "\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(BasicConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "GoogLeNetOutputs = namedtuple('GoogLeNetOutputs', ['logits', 'aux_logits2', 'aux_logits1'])\n",
        "GoogLeNetOutputs.__annotations__ = {'logits': Tensor, 'aux_logits2': Optional[Tensor],\n",
        "                                    'aux_logits1': Optional[Tensor]}\n",
        "_GoogLeNetOutputs = GoogLeNetOutputs\n",
        "\n",
        "class GoogLeNet(nn.Module):\n",
        "    __constants__ = ['aux_logits']\n",
        "\n",
        "    def __init__(self, num_classes=50, aux_logits=True, init_weights=True, blocks=None):\n",
        "        super(GoogLeNet, self).__init__()\n",
        "        if blocks is None:\n",
        "            blocks = [BasicConv2d, Inception, InceptionAux]\n",
        "        assert len(blocks) == 3\n",
        "        conv_block = blocks[0]\n",
        "        inception_block = blocks[1]\n",
        "        inception_aux_block = blocks[2]\n",
        "\n",
        "        self.aux_logits = aux_logits\n",
        "\n",
        "        self.features1 = nn.Sequential(OrderedDict([\n",
        "          ('conv1', conv_block(3, 64, kernel_size=7, stride=2, padding=3)),\n",
        "          ('maxpool1', nn.MaxPool2d(3, stride=2, ceil_mode=True)),\n",
        "          ('conv2', conv_block(64, 64, kernel_size=1)),\n",
        "          ('conv3', conv_block(64, 192, kernel_size=3, padding=1)),\n",
        "          ('maxpool2', nn.MaxPool2d(3, stride=2, ceil_mode=True)),\n",
        "        ]))\n",
        "\n",
        "        self.inception3 = nn.Sequential(OrderedDict([\n",
        "          ('incep3a', inception_block(192, 64, 96, 128, 16, 32, 32)),     \n",
        "          ('incep3b', inception_block(256, 128, 128, 192, 32, 96, 64)),  \n",
        "          ('maxpool3', nn.MaxPool2d(3, stride=2, ceil_mode=True)),        \n",
        "        ]))\n",
        "\n",
        "        self.inception4a = inception_block(480, 192, 96, 208, 16, 48, 64)\n",
        "        self.inception4b = inception_block(512, 160, 112, 224, 24, 64, 64)\n",
        "        self.inception4c = inception_block(512, 128, 128, 256, 24, 64, 64)\n",
        "        self.inception4d = inception_block(512, 112, 144, 288, 32, 64, 64)\n",
        "        self.inception4e = inception_block(528, 256, 160, 320, 32, 128, 128)\n",
        "\n",
        "        self.extraconv = nn.Conv2d(832, 1024, kernel_size=3, padding=1)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "        self.classifier = nn.Sequential(OrderedDict([\n",
        "          ('extrafc', nn.Linear(1024 * 1 * 1, num_classes)),\n",
        "          ('softmax', nn.Softmax(dim=1))                                       \n",
        "        ]))\n",
        "\n",
        "        if aux_logits:\n",
        "            self.aux1 = inception_aux_block(512, num_classes)\n",
        "            self.aux2 = inception_aux_block(528, num_classes)\n",
        "        else:\n",
        "            self.aux1 = None\n",
        "            self.aux2 = None\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                import scipy.stats as stats\n",
        "                X = stats.truncnorm(-2, 2, scale=0.01)\n",
        "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
        "                values = values.view(m.weight.size())\n",
        "                with torch.no_grad():\n",
        "                    m.weight.copy_(values)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _forward(self, x):        \n",
        "        # type: (Tensor) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]       \n",
        "        x = self.features1(x)       # Out: N x 192 x 28 x 28\n",
        "        x = self.inception3(x)      # Out: N x 480 x 14 x 14\n",
        "        x = self.inception4a(x)     # Out: N x 512 x 14 x 14       \n",
        "        aux1 = torch.jit.annotate(Optional[Tensor], None) \n",
        "        if self.aux1 is not None:\n",
        "            if self.training:\n",
        "                aux1 = self.aux1(x)\n",
        "\n",
        "        x = self.inception4b(x)     # Out: N x 512 x 14 x 14\n",
        "        x = self.inception4c(x)     # Out: N x 512 x 14 x 14        \n",
        "        x = self.inception4d(x)     # Out: N x 528 x 14 x 14       \n",
        "        aux2 = torch.jit.annotate(Optional[Tensor], None) \n",
        "        if self.aux2 is not None:\n",
        "            if self.training:\n",
        "                aux2 = self.aux2(x)\n",
        "\n",
        "        x = self.inception4e(x)     # Out: N x 832 x 14 x 14\n",
        "        x = self.extraconv(x)\n",
        "        feat_maps = x\n",
        "        x = self.avgool(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.classifier(x)      \n",
        "        return x, aux2, aux1, feat_maps\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def eager_outputs(self, x, aux2, aux1):\n",
        "        # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> GoogLeNetOutputs\n",
        "        if self.training and self.aux_logits:\n",
        "            return _GoogLeNetOutputs(x, aux2, aux1)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # type: (Tensor) -> GoogLeNetOutputs\n",
        "        x, aux1, aux2, feat_maps = self._forward(x)\n",
        "        aux_defined = self.training and self.aux_logits\n",
        "        if torch.jit.is_scripting():\n",
        "            if not aux_defined:\n",
        "                warnings.warn(\"Scripted GoogleNet always returns GoogleNetOutputs Tuple\")\n",
        "            return GoogLeNetOutputs(x, aux2, aux1), feat_maps\n",
        "        else:\n",
        "            return self.eager_outputs(x, aux2, aux1), feat_maps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDgMWNS8JYZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
        "    # generate the class activation maps upsample to 224x224\n",
        "    size_upsample = (224,224)\n",
        "    bz, nc, h, w = feature_conv.shape\n",
        "    output_cam = []\n",
        "    for idx in class_idx:\n",
        "        cam = torch.matmul(weight_softmax[idx], feature_conv.view((nc, h*w)))\n",
        "        cam = cam.view(h, w)\n",
        "        cam = cam - torch.min(cam)\n",
        "        cam_img = cam / torch.max(cam)\n",
        "        cam_img = 255*cam_img\n",
        "        cam_img = cam_img.to(torch.uint8)\n",
        "        output_cam.append(cv2.resize(cam_img.numpy(), size_upsample))\n",
        "    return output_cam\n",
        "\n",
        "\n",
        "#net123 = 'AlexNet'\n",
        "#alexnet = globals()[net123]()\n",
        "\n",
        "# alexnet = AlexNet()\n",
        "# googlenet = GoogLeNet()\n",
        "\n",
        "# load test image of mountain\n",
        "# x = Image.open(os.path.join(path, '3.png'))\n",
        "# x = transforms.ToTensor()(x)[:3,:,:].view(-1,3,224,224)\n",
        "\n",
        "# Outputs AlexNet\n",
        "# output, _ = alexnet(x)\n",
        "# weight_fc_alex = alexnet.classifier[0].weight\n",
        "\n",
        "# # Outputs GoogLeNet\n",
        "#output, feat_maps_google = googlenet(x)           # output is namedtuple\n",
        "# weight_fc_google = googlenet.classifier[0].weight\n",
        "\n",
        "# idx = range(1000)\n",
        "# # generate class activation mapping for the top1 prediction\n",
        "# CAMs_alex = returnCAM(feat_maps_alex, weight_fc_alex, [idx[0]])\n",
        "# CAMs_google = returnCAM(feat_maps_google, weight_fc_google, [idx[0]])\n",
        "# #print(CAMs_alex.shape)\n",
        "# #print(CAMs_google.shape)\n",
        "      \n",
        "# # render the CAM and output\n",
        "# # print('output CAM.png for the top1 prediction: %s'%classes[idx[0]])\n",
        "# img = cv2.imread('3.png')\n",
        "# height, width, _ = img.shape\n",
        "# heatmap_al = cv2.applyColorMap(cv2.resize(CAMs_alex[0], (width, height)), cv2.COLORMAP_JET)\n",
        "# result_al = heatmap_al * 0.3 + img * 0.5\n",
        "# cv2.imwrite('CAM alexnet.png', result_al)\n",
        "# heatmap_go = cv2.applyColorMap(cv2.resize(CAMs_google[0], (width, height)), cv2.COLORMAP_JET)\n",
        "# result_go = heatmap_go * 0.3 + img * 0.5\n",
        "# cv2.imwrite('CAM google.png', result_go)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmOxizfJpa85",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## Data Prep: Hide a Patch\n",
        "\n",
        "In order to train the network, certain patches of the training images are hidden with probability p = 0.5 ...........\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHpRSZ5Za0jG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def hide_patch(img):\n",
        "    # get width and height of the image\n",
        "    s = img.shape\n",
        "    wd = s[1]\n",
        "    ht = s[2]\n",
        "\n",
        "    # possible grid size, 0 means no hiding\n",
        "    grid_sizes=[0,16,32,44,56]\n",
        "\n",
        "    # hiding probability\n",
        "    hide_prob = 0.5\n",
        "\n",
        "    # randomly choose one grid size\n",
        "    grid_size= grid_sizes[random.randint(0,len(grid_sizes)-1)]\n",
        "\n",
        "    # hide the patches \n",
        "    if grid_size > 0:\n",
        "        for x in range(0, wd, grid_size):\n",
        "            for y in range(0, ht, grid_size):\n",
        "                x_end = min(wd, x + grid_size)  \n",
        "                y_end = min(ht, y + grid_size)\n",
        "                if random.random() <=  hide_prob:\n",
        "                      img[0, x:x_end, y:y_end] = 0.4639      # Hides patch to zero, but has to be mean of dataset!!!!!\n",
        "                      img[1, x:x_end, y:y_end] = 0.4415\n",
        "                      img[2, x:x_end, y:y_end] = 0.3947\n",
        "    return img\n",
        "              \n",
        "# z = Image.open(os.path.join(path, '3.png'))\n",
        "# z = transforms.ToTensor()(z)[:3,:,:].view(-1,3,224,224)\n",
        "# batchsize = 1\n",
        "# for i in range(batchsize):\n",
        "#   img = hide_patch(z[i])\n",
        "\n",
        "# plt.imshow(img.permute(1,2,0))\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1KX4pxKeBEY",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## Model Hyperparameter setting\n",
        "\n",
        "Blah blah blah\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC3ukVNJcdPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelArgs:\n",
        "   \"\"\"\n",
        "    Passing the hyperparameters to the model\n",
        "   \"\"\"\n",
        "   def __init__(self, netType='alexnet', hideProb=0.5, validate=0, pretrained=0, patchsize=16, nClasses=50, \n",
        "                epochs=55, start_epoch=0, batch_size=128, lr=0.01, momentum=0.9, print_freq=30, \n",
        "                weight_decay=5e-4, save_dir='save_trained', save_every=10, inputsize=224):\n",
        "        self.save_every = save_every  # Saves checkpoints at every specified number of epochs\n",
        "        self.save_dir = save_dir      # The directory used to save the trained models\n",
        "        self.validate = validate      # validate model on validation set\n",
        "        self.pretrained = pretrained\n",
        "        self.weight_decay = weight_decay\n",
        "        self.momentum = momentum\n",
        "        self.lr = lr                  # Learning rate\n",
        "        self.print_freq = print_freq\n",
        "        self.batch_size = batch_size\n",
        "        self.start_epoch = start_epoch\n",
        "        self.epochs = epochs\n",
        "        self.nClasses = nClasses      # number of classes\n",
        "        self.patchsize = patchsize    # size of patches\n",
        "        self.hideProb = hideProb      # probability of hiding patch\n",
        "        self.netType = netType        # type of neural net\n",
        "        self.inputsize = inputsize    # alex = 227 and google = 224\n",
        "\n",
        "alexargs = ModelArgs('alexnet', epochs=55, inputsize=227)\n",
        "googleargs = ModelArgs('googlenet', epochs=40, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jO5tMTUkpAU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Import training and test images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s8sDgcCk1Y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "im_path= os.path.join(path, 'data')\n",
        "\n",
        "def load_images(modelargs=alexargs):\n",
        "    trans = transforms.Compose([transforms.Resize((modelargs.inputsize,modelargs.inputsize)), \n",
        "                                transforms.ToTensor()])\n",
        "    trainset = datasets.ImageFolder(os.path.join(im_path, 'train'), \n",
        "                                    transform=trans)\n",
        "    return torch.utils.data.DataLoader(trainset, batch_size=modelargs.batch_size, num_workers=4, pin_memory=True, shuffle=True)\n",
        "\n",
        "\n",
        "def load_valims(modelargs=alexargs):\n",
        "    trans = transforms.Compose([transforms.Resize((modelargs.inputsize,modelargs.inputsize)), \n",
        "                                transforms.ToTensor()])\n",
        "    valset = datasets.ImageFolder(os.path.join(im_path, 'test'), \n",
        "                                  transform=trans)\n",
        "    return torch.utils.data.DataLoader(valset, batch_size=modelargs.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "#images, labels = next(iter(train_loader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PorwHiMVIvgl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Training and Validation\n",
        "\n",
        "\"We train the network from scratch for 55 and 40 epochs for AlexNet-GAP and GoogLeNet=GAP, respectively, with a batch size of 128 and initial learning rate of 0.01. We gradually decrease the learning rate to 0.0001.\" \n",
        "\n",
        "**AlexNet:**\n",
        "\"We trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. \"\n",
        "\n",
        "**GoogLeNet:**\n",
        "\"Our training used asynchronous stochastic gradient descent with 0.9 momentum [17], fixed learning rate schedule (decreasing the learning rate by 4% every 8 epochs). Polyak averaging [13] was used to create the final model used at inference time.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4hu7f8dI15v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, args, epoch=1, HaS=False):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for idx, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Hide patches\n",
        "        if HaS:\n",
        "          for input in inputs:\n",
        "            input = hide_patch(input)\n",
        "            \n",
        "        \n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        input_var = inputs.cuda()\n",
        "        # target = labels.cuda()\n",
        "        # target_var = target.cuda()\n",
        "        target_var = labels.cuda()\n",
        "\n",
        "        # compute output\n",
        "        if args == alexargs:\n",
        "          outputs, _ = model(input_var)\n",
        "          loss = criterion(outputs, target_var)\n",
        "        elif args == googleargs:\n",
        "          outputs, _ = model(input_var)\n",
        "          loss = criterion(outputs.logits, target_var) + 0.3*criterion(outputs.aux_logits2, target_var) + 0.3*criterion(outputs.aux_logits1, target_var)\n",
        "          outputs = outputs.logits \n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = outputs.float()\n",
        "        loss = loss.float()\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(outputs.data, target_var)[0]       # was target\n",
        "        prec5 = accuracy(outputs.data, target_var)[1]       # was target\n",
        "        losses.update(loss.item(), inputs.size(0))  \n",
        "        top1.update(prec1.item(), inputs.size(0))\n",
        "        top5.update(prec5.item(), inputs.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if idx % args.print_freq == 0:\n",
        "            print('Epoch: [{}][{}/{}]\\t'\n",
        "                  'Loss {:.4f} ({:.4f})\\t'\n",
        "                  'Prec@1 {:.3f} ({:.3f})\\t'\n",
        "                  'Prec@5 {:.3f} ({:.3f})\\t batch time: {:.2f}'.format(\n",
        "                      epoch, idx, len(train_loader), losses.val, losses.avg, top1.val, top1.avg, top5.val, top5.avg, batch_time.val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gqS8eEojhuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(val_loader):\n",
        "            inputs, labels = data\n",
        "\n",
        "            target = labels.cuda()\n",
        "            input_var = inputs.cuda()\n",
        "            target_var = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            outputs, _ = model(input_var)           # ignore featmaps output\n",
        "            loss = criterion(outputs, target_var)\n",
        "\n",
        "            outputs = outputs.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(outputs.data, target)[0]\n",
        "            prec5 = accuracy(outputs.data, target)[1]\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            top1.update(prec1.item(), inputs.size(0))\n",
        "            top5.update(prec5.item(), inputs.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "\n",
        "    print('Test\\t  Prec@1: {top1.avg:.3f}\\t Prec@5: {top5.avg:.3f} (Err: {error:.3f} )\\n'\n",
        "            .format(top1=top1,top5=top5,error=100-top1.avg))\n",
        "\n",
        "    return top1.avg\n",
        "\n",
        "def save_checkpoint(state, filename='checkpoint.th'):\n",
        "    \"\"\"Save the training model\"\"\"\n",
        "    torch.save(state, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UICpYdurI0Rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,5)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvT1BKPtQgKK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Running the Experiment\n",
        "\n",
        "Learning rate decay milestones = [18 29 43 52 1e8] by 0.5 and start 1e-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsJ4ODqtUqPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def main(args, model, HaS=True):\n",
        "    global best_prec1\n",
        "    \n",
        "    # Check the save_dir exists or not\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "\n",
        "    model.cuda()\n",
        "\n",
        "    # define loss function (criterion) and pptimizer\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
        "                                momentum=args.momentum,\n",
        "                                weight_decay=args.weight_decay)\n",
        "\n",
        "    lr_schedule = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                        milestones=[18, 29, 43, 52, 1e8], last_epoch=args.start_epoch - 1)\n",
        "\n",
        "    if args.validate:\n",
        "        print('evalution mode')\n",
        "        model.load_state_dict(torch.load(os.path.join(args.save_dir, '%s.th' %args.netType)))\n",
        "        best_prec1 = validate(load_valims(args), model, criterion)\n",
        "        return best_prec1\n",
        "\n",
        "    if args.pretrained:\n",
        "        print('evalution of pretrained model')\n",
        "        args.save_dir='pretrained_models'\n",
        "        pretrained_model= '%s.th' %args.netType\n",
        "        model.load_state_dict(torch.load(os.path.join(args.save_dir, pretrained_model)))\n",
        "        best_prec1 = validate(load_valims(args), model, criterion)\n",
        "        return best_prec1\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "\n",
        "        # train for one epoch\n",
        "        print('Training {} model'.format(args.netType))\n",
        "        print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
        "        train(load_images(args), model, criterion, optimizer, args, epoch, HaS)\n",
        "        lr_schedule.step()\n",
        "\n",
        "        # evaluate on validation set\n",
        "        prec1 = validate(load_valims(args), model, criterion)\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = prec1 > best_prec1          # Boolean \n",
        "        best_prec1 = max(prec1, best_prec1)\n",
        "\n",
        "        if epoch > 0 and epoch % args.save_every == 0:\n",
        "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, '%s_checkp.th' %args.netType))\n",
        "        if is_best:\n",
        "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, '%s.th' %args.netType))\n",
        "\n",
        "    return best_prec1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V49Wzb7uYW-B",
        "colab_type": "code",
        "outputId": "7b73ad99-6515-49f9-d4c6-30820fa8beaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "best_prec1 = 0\n",
        "best_prec1 = main(googleargs, GoogLeNet())\n",
        "print('The lowest error from {} model after {} epochs is {error:.3f}'.format(alexargs.netType,alexargs.epochs,error=100-best_prec1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [0][0/370]\tLoss 6.2592 (6.2592)\tPrec@1 0.781 (0.781)\tPrec@5 9.375 (9.375)\t batch time: 50.45\n",
            "Epoch: [0][30/370]\tLoss 6.2591 (6.2590)\tPrec@1 2.344 (1.865)\tPrec@5 8.594 (12.475)\t batch time: 2.38\n",
            "Epoch: [0][60/370]\tLoss 6.2588 (6.2587)\tPrec@1 0.000 (2.024)\tPrec@5 13.281 (13.525)\t batch time: 42.50\n",
            "Epoch: [0][90/370]\tLoss 6.2538 (6.2576)\tPrec@1 2.344 (1.983)\tPrec@5 20.312 (14.269)\t batch time: 1.02\n",
            "Epoch: [0][120/370]\tLoss 6.2530 (6.2558)\tPrec@1 0.781 (1.937)\tPrec@5 17.188 (14.598)\t batch time: 30.94\n",
            "Epoch: [0][150/370]\tLoss 6.2566 (6.2537)\tPrec@1 1.562 (2.178)\tPrec@5 14.062 (14.580)\t batch time: 1.03\n",
            "Epoch: [0][180/370]\tLoss 6.2408 (6.2507)\tPrec@1 3.125 (2.512)\tPrec@5 14.062 (14.801)\t batch time: 1.05\n",
            "Epoch: [0][210/370]\tLoss 6.2248 (6.2485)\tPrec@1 5.469 (2.814)\tPrec@5 18.750 (15.070)\t batch time: 1.04\n",
            "Epoch: [0][240/370]\tLoss 6.2368 (6.2469)\tPrec@1 4.688 (3.063)\tPrec@5 12.500 (15.447)\t batch time: 1.05\n",
            "Epoch: [0][270/370]\tLoss 6.2253 (6.2452)\tPrec@1 5.469 (3.350)\tPrec@5 17.188 (15.925)\t batch time: 1.06\n",
            "Epoch: [0][300/370]\tLoss 6.2576 (6.2435)\tPrec@1 1.562 (3.584)\tPrec@5 19.531 (16.271)\t batch time: 1.03\n",
            "Epoch: [0][330/370]\tLoss 6.2301 (6.2431)\tPrec@1 6.250 (3.736)\tPrec@5 21.094 (16.491)\t batch time: 1.05\n",
            "Epoch: [0][360/370]\tLoss 6.1902 (6.2414)\tPrec@1 8.594 (3.986)\tPrec@5 21.094 (16.776)\t batch time: 1.01\n",
            "Test\t  Prec@1: 6.190\t Prec@5: 19.790 (Err: 93.810 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [1][0/370]\tLoss 6.1849 (6.1849)\tPrec@1 9.375 (9.375)\tPrec@5 21.875 (21.875)\t batch time: 4.77\n",
            "Epoch: [1][30/370]\tLoss 6.1931 (6.2107)\tPrec@1 8.594 (7.031)\tPrec@5 32.031 (21.749)\t batch time: 1.34\n",
            "Epoch: [1][60/370]\tLoss 6.2243 (6.2119)\tPrec@1 7.031 (6.942)\tPrec@5 21.094 (21.619)\t batch time: 1.18\n",
            "Epoch: [1][90/370]\tLoss 6.2372 (6.2120)\tPrec@1 4.688 (6.834)\tPrec@5 26.562 (22.124)\t batch time: 1.29\n",
            "Epoch: [1][120/370]\tLoss 6.2108 (6.2110)\tPrec@1 7.031 (6.883)\tPrec@5 25.000 (22.411)\t batch time: 1.23\n",
            "Epoch: [1][150/370]\tLoss 6.1729 (6.2103)\tPrec@1 9.375 (6.886)\tPrec@5 22.656 (22.749)\t batch time: 1.23\n",
            "Epoch: [1][180/370]\tLoss 6.1805 (6.2112)\tPrec@1 10.156 (6.790)\tPrec@5 29.688 (22.764)\t batch time: 1.14\n",
            "Epoch: [1][210/370]\tLoss 6.2041 (6.2103)\tPrec@1 7.812 (6.876)\tPrec@5 26.562 (23.012)\t batch time: 1.22\n",
            "Epoch: [1][240/370]\tLoss 6.2423 (6.2091)\tPrec@1 5.469 (6.970)\tPrec@5 25.000 (23.519)\t batch time: 1.18\n",
            "Epoch: [1][270/370]\tLoss 6.1871 (6.2074)\tPrec@1 7.812 (7.161)\tPrec@5 25.781 (23.951)\t batch time: 1.26\n",
            "Epoch: [1][300/370]\tLoss 6.2036 (6.2063)\tPrec@1 7.031 (7.221)\tPrec@5 22.656 (24.182)\t batch time: 1.34\n",
            "Epoch: [1][330/370]\tLoss 6.1821 (6.2049)\tPrec@1 11.719 (7.340)\tPrec@5 29.688 (24.424)\t batch time: 1.41\n",
            "Epoch: [1][360/370]\tLoss 6.1787 (6.2030)\tPrec@1 9.375 (7.518)\tPrec@5 28.906 (24.695)\t batch time: 1.21\n",
            "Test\t  Prec@1: 7.390\t Prec@5: 23.086 (Err: 92.610 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [2][0/370]\tLoss 6.1873 (6.1873)\tPrec@1 7.812 (7.812)\tPrec@5 25.781 (25.781)\t batch time: 4.32\n",
            "Epoch: [2][30/370]\tLoss 6.1637 (6.1899)\tPrec@1 9.375 (8.543)\tPrec@5 27.344 (26.764)\t batch time: 1.18\n",
            "Epoch: [2][60/370]\tLoss 6.1657 (6.1850)\tPrec@1 10.156 (8.863)\tPrec@5 27.344 (26.806)\t batch time: 1.19\n",
            "Epoch: [2][90/370]\tLoss 6.1783 (6.1780)\tPrec@1 9.375 (9.530)\tPrec@5 28.906 (27.163)\t batch time: 1.25\n",
            "Epoch: [2][120/370]\tLoss 6.1789 (6.1756)\tPrec@1 10.156 (9.756)\tPrec@5 22.656 (27.157)\t batch time: 1.16\n",
            "Epoch: [2][150/370]\tLoss 6.1347 (6.1702)\tPrec@1 14.062 (10.265)\tPrec@5 32.812 (27.302)\t batch time: 1.30\n",
            "Epoch: [2][180/370]\tLoss 6.1722 (6.1685)\tPrec@1 9.375 (10.320)\tPrec@5 29.688 (27.335)\t batch time: 1.22\n",
            "Epoch: [2][210/370]\tLoss 6.1074 (6.1666)\tPrec@1 14.844 (10.427)\tPrec@5 27.344 (27.536)\t batch time: 1.21\n",
            "Epoch: [2][240/370]\tLoss 6.2004 (6.1655)\tPrec@1 7.031 (10.523)\tPrec@5 28.906 (27.648)\t batch time: 1.22\n",
            "Epoch: [2][270/370]\tLoss 6.1521 (6.1643)\tPrec@1 10.156 (10.626)\tPrec@5 26.562 (27.923)\t batch time: 1.31\n",
            "Epoch: [2][300/370]\tLoss 6.1838 (6.1631)\tPrec@1 7.812 (10.743)\tPrec@5 25.000 (28.050)\t batch time: 1.21\n",
            "Epoch: [2][330/370]\tLoss 6.1650 (6.1617)\tPrec@1 11.719 (10.921)\tPrec@5 34.375 (28.175)\t batch time: 1.17\n",
            "Epoch: [2][360/370]\tLoss 6.1020 (6.1599)\tPrec@1 15.625 (11.137)\tPrec@5 30.469 (28.238)\t batch time: 1.32\n",
            "Test\t  Prec@1: 12.667\t Prec@5: 28.838 (Err: 87.333 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [3][0/370]\tLoss 6.0950 (6.0950)\tPrec@1 17.188 (17.188)\tPrec@5 42.188 (42.188)\t batch time: 4.98\n",
            "Epoch: [3][30/370]\tLoss 6.1389 (6.1495)\tPrec@1 13.281 (12.349)\tPrec@5 24.219 (30.217)\t batch time: 1.31\n",
            "Epoch: [3][60/370]\tLoss 6.0996 (6.1445)\tPrec@1 17.969 (12.846)\tPrec@5 33.594 (30.200)\t batch time: 1.21\n",
            "Epoch: [3][90/370]\tLoss 6.0986 (6.1395)\tPrec@1 16.406 (13.204)\tPrec@5 37.500 (30.271)\t batch time: 1.26\n",
            "Epoch: [3][120/370]\tLoss 6.2072 (6.1384)\tPrec@1 7.031 (13.223)\tPrec@5 18.750 (30.139)\t batch time: 1.12\n",
            "Epoch: [3][150/370]\tLoss 6.1326 (6.1363)\tPrec@1 13.281 (13.462)\tPrec@5 34.375 (30.153)\t batch time: 1.26\n",
            "Epoch: [3][180/370]\tLoss 6.0841 (6.1326)\tPrec@1 19.531 (13.747)\tPrec@5 35.938 (30.348)\t batch time: 1.30\n",
            "Epoch: [3][210/370]\tLoss 6.1174 (6.1308)\tPrec@1 16.406 (13.885)\tPrec@5 35.938 (30.269)\t batch time: 1.20\n",
            "Epoch: [3][240/370]\tLoss 6.1102 (6.1289)\tPrec@1 16.406 (14.033)\tPrec@5 30.469 (30.336)\t batch time: 1.24\n",
            "Epoch: [3][270/370]\tLoss 6.1529 (6.1272)\tPrec@1 11.719 (14.175)\tPrec@5 28.906 (30.503)\t batch time: 1.27\n",
            "Epoch: [3][300/370]\tLoss 6.0788 (6.1258)\tPrec@1 19.531 (14.249)\tPrec@5 33.594 (30.612)\t batch time: 1.34\n",
            "Epoch: [3][330/370]\tLoss 6.1119 (6.1253)\tPrec@1 16.406 (14.258)\tPrec@5 35.156 (30.672)\t batch time: 1.24\n",
            "Epoch: [3][360/370]\tLoss 6.0464 (6.1244)\tPrec@1 22.656 (14.352)\tPrec@5 35.156 (30.772)\t batch time: 1.19\n",
            "Test\t  Prec@1: 12.457\t Prec@5: 24.533 (Err: 87.543 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [4][0/370]\tLoss 6.0922 (6.0922)\tPrec@1 16.406 (16.406)\tPrec@5 32.812 (32.812)\t batch time: 4.27\n",
            "Epoch: [4][30/370]\tLoss 6.1470 (6.1135)\tPrec@1 12.500 (15.600)\tPrec@5 28.125 (31.174)\t batch time: 1.20\n",
            "Epoch: [4][60/370]\tLoss 6.1099 (6.1108)\tPrec@1 14.844 (16.099)\tPrec@5 30.469 (31.660)\t batch time: 1.36\n",
            "Epoch: [4][90/370]\tLoss 6.0887 (6.1091)\tPrec@1 18.750 (16.140)\tPrec@5 31.250 (31.156)\t batch time: 1.23\n",
            "Epoch: [4][120/370]\tLoss 6.1596 (6.1109)\tPrec@1 10.938 (15.799)\tPrec@5 22.656 (30.572)\t batch time: 1.22\n",
            "Epoch: [4][150/370]\tLoss 6.1073 (6.1070)\tPrec@1 17.188 (16.049)\tPrec@5 28.906 (30.805)\t batch time: 1.23\n",
            "Epoch: [4][180/370]\tLoss 6.1715 (6.1041)\tPrec@1 10.938 (16.307)\tPrec@5 25.781 (31.017)\t batch time: 1.28\n",
            "Epoch: [4][210/370]\tLoss 6.0631 (6.1018)\tPrec@1 19.531 (16.447)\tPrec@5 39.844 (31.120)\t batch time: 1.32\n",
            "Epoch: [4][240/370]\tLoss 6.0639 (6.0978)\tPrec@1 18.750 (16.750)\tPrec@5 32.812 (31.493)\t batch time: 1.25\n",
            "Epoch: [4][270/370]\tLoss 6.1079 (6.0974)\tPrec@1 14.844 (16.769)\tPrec@5 33.594 (31.639)\t batch time: 1.20\n",
            "Epoch: [4][300/370]\tLoss 6.1152 (6.0980)\tPrec@1 14.844 (16.754)\tPrec@5 31.250 (31.616)\t batch time: 1.26\n",
            "Epoch: [4][330/370]\tLoss 6.0881 (6.0973)\tPrec@1 17.969 (16.843)\tPrec@5 29.688 (31.727)\t batch time: 1.26\n",
            "Epoch: [4][360/370]\tLoss 6.1144 (6.0970)\tPrec@1 15.625 (16.869)\tPrec@5 29.688 (31.769)\t batch time: 1.19\n",
            "Test\t  Prec@1: 15.790\t Prec@5: 31.029 (Err: 84.210 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [5][0/370]\tLoss 6.0843 (6.0843)\tPrec@1 17.969 (17.969)\tPrec@5 29.688 (29.688)\t batch time: 4.34\n",
            "Epoch: [5][30/370]\tLoss 6.0692 (6.0876)\tPrec@1 20.312 (17.843)\tPrec@5 34.375 (31.830)\t batch time: 1.18\n",
            "Epoch: [5][60/370]\tLoss 6.1049 (6.0897)\tPrec@1 16.406 (17.508)\tPrec@5 32.031 (31.532)\t batch time: 1.21\n",
            "Epoch: [5][90/370]\tLoss 6.0547 (6.0893)\tPrec@1 21.875 (17.436)\tPrec@5 35.938 (32.297)\t batch time: 1.21\n",
            "Epoch: [5][120/370]\tLoss 6.0862 (6.0867)\tPrec@1 17.969 (17.665)\tPrec@5 30.469 (32.812)\t batch time: 1.12\n",
            "Epoch: [5][150/370]\tLoss 6.0515 (6.0865)\tPrec@1 21.094 (17.705)\tPrec@5 39.062 (33.071)\t batch time: 1.43\n",
            "Epoch: [5][180/370]\tLoss 5.9843 (6.0861)\tPrec@1 28.906 (17.861)\tPrec@5 39.844 (33.201)\t batch time: 1.31\n",
            "Epoch: [5][210/370]\tLoss 6.0986 (6.0860)\tPrec@1 16.406 (17.891)\tPrec@5 29.688 (33.153)\t batch time: 1.17\n",
            "Epoch: [5][240/370]\tLoss 6.1612 (6.0844)\tPrec@1 10.938 (18.043)\tPrec@5 30.469 (33.279)\t batch time: 1.18\n",
            "Epoch: [5][270/370]\tLoss 6.0685 (6.0835)\tPrec@1 20.312 (18.153)\tPrec@5 39.844 (33.357)\t batch time: 1.20\n",
            "Epoch: [5][300/370]\tLoss 6.1456 (6.0820)\tPrec@1 14.062 (18.288)\tPrec@5 29.688 (33.490)\t batch time: 1.21\n",
            "Epoch: [5][330/370]\tLoss 6.0343 (6.0809)\tPrec@1 21.875 (18.408)\tPrec@5 31.250 (33.521)\t batch time: 1.29\n",
            "Epoch: [5][360/370]\tLoss 6.0457 (6.0799)\tPrec@1 23.438 (18.527)\tPrec@5 41.406 (33.819)\t batch time: 1.18\n",
            "Test\t  Prec@1: 13.886\t Prec@5: 29.352 (Err: 86.114 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [6][0/370]\tLoss 6.0635 (6.0635)\tPrec@1 17.969 (17.969)\tPrec@5 36.719 (36.719)\t batch time: 4.42\n",
            "Epoch: [6][30/370]\tLoss 6.0116 (6.0689)\tPrec@1 25.000 (19.405)\tPrec@5 34.375 (35.811)\t batch time: 1.29\n",
            "Epoch: [6][60/370]\tLoss 6.0058 (6.0720)\tPrec@1 27.344 (19.301)\tPrec@5 41.406 (35.733)\t batch time: 1.25\n",
            "Epoch: [6][90/370]\tLoss 6.0582 (6.0754)\tPrec@1 19.531 (18.896)\tPrec@5 45.312 (35.723)\t batch time: 1.18\n",
            "Epoch: [6][120/370]\tLoss 6.1382 (6.0719)\tPrec@1 13.281 (19.247)\tPrec@5 27.344 (35.841)\t batch time: 1.19\n",
            "Epoch: [6][150/370]\tLoss 6.0803 (6.0699)\tPrec@1 17.969 (19.412)\tPrec@5 41.406 (36.005)\t batch time: 1.22\n",
            "Epoch: [6][180/370]\tLoss 6.0241 (6.0692)\tPrec@1 24.219 (19.505)\tPrec@5 35.156 (36.041)\t batch time: 1.28\n",
            "Epoch: [6][210/370]\tLoss 6.0617 (6.0683)\tPrec@1 21.094 (19.583)\tPrec@5 39.062 (36.108)\t batch time: 1.18\n",
            "Epoch: [6][240/370]\tLoss 6.0766 (6.0663)\tPrec@1 17.188 (19.800)\tPrec@5 35.156 (36.232)\t batch time: 1.27\n",
            "Epoch: [6][270/370]\tLoss 6.0065 (6.0639)\tPrec@1 24.219 (20.007)\tPrec@5 35.156 (36.442)\t batch time: 1.18\n",
            "Epoch: [6][300/370]\tLoss 6.0882 (6.0625)\tPrec@1 18.750 (20.133)\tPrec@5 41.406 (36.493)\t batch time: 1.23\n",
            "Epoch: [6][330/370]\tLoss 5.9774 (6.0613)\tPrec@1 27.344 (20.237)\tPrec@5 39.844 (36.615)\t batch time: 1.19\n",
            "Epoch: [6][360/370]\tLoss 6.0428 (6.0607)\tPrec@1 19.531 (20.291)\tPrec@5 38.281 (36.554)\t batch time: 1.31\n",
            "Test\t  Prec@1: 15.486\t Prec@5: 31.048 (Err: 84.514 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [7][0/370]\tLoss 6.0796 (6.0796)\tPrec@1 18.750 (18.750)\tPrec@5 41.406 (41.406)\t batch time: 4.68\n",
            "Epoch: [7][30/370]\tLoss 6.0558 (6.0510)\tPrec@1 19.531 (20.968)\tPrec@5 29.688 (36.517)\t batch time: 1.26\n",
            "Epoch: [7][60/370]\tLoss 6.0766 (6.0370)\tPrec@1 17.188 (22.477)\tPrec@5 31.250 (37.795)\t batch time: 1.16\n",
            "Epoch: [7][90/370]\tLoss 6.0320 (6.0338)\tPrec@1 22.656 (22.802)\tPrec@5 36.719 (38.187)\t batch time: 1.24\n",
            "Epoch: [7][120/370]\tLoss 5.9878 (6.0349)\tPrec@1 27.344 (22.766)\tPrec@5 39.062 (37.881)\t batch time: 1.21\n",
            "Epoch: [7][150/370]\tLoss 6.0964 (6.0329)\tPrec@1 16.406 (22.853)\tPrec@5 32.812 (38.007)\t batch time: 1.16\n",
            "Epoch: [7][180/370]\tLoss 6.0431 (6.0306)\tPrec@1 22.656 (23.032)\tPrec@5 35.156 (38.130)\t batch time: 1.19\n",
            "Epoch: [7][210/370]\tLoss 6.0470 (6.0295)\tPrec@1 20.312 (23.060)\tPrec@5 35.938 (38.189)\t batch time: 1.21\n",
            "Epoch: [7][240/370]\tLoss 6.0015 (6.0284)\tPrec@1 26.562 (23.117)\tPrec@5 40.625 (38.152)\t batch time: 1.26\n",
            "Epoch: [7][270/370]\tLoss 6.0252 (6.0281)\tPrec@1 23.438 (23.167)\tPrec@5 39.062 (38.232)\t batch time: 1.15\n",
            "Epoch: [7][300/370]\tLoss 5.9916 (6.0277)\tPrec@1 25.000 (23.196)\tPrec@5 38.281 (38.227)\t batch time: 1.25\n",
            "Epoch: [7][330/370]\tLoss 6.0098 (6.0280)\tPrec@1 23.438 (23.166)\tPrec@5 35.156 (38.177)\t batch time: 1.12\n",
            "Epoch: [7][360/370]\tLoss 5.9259 (6.0285)\tPrec@1 29.688 (23.100)\tPrec@5 46.094 (38.069)\t batch time: 1.16\n",
            "Test\t  Prec@1: 19.010\t Prec@5: 36.381 (Err: 80.990 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [8][0/370]\tLoss 5.9736 (5.9736)\tPrec@1 28.125 (28.125)\tPrec@5 40.625 (40.625)\t batch time: 4.50\n",
            "Epoch: [8][30/370]\tLoss 5.8929 (6.0144)\tPrec@1 34.375 (24.244)\tPrec@5 43.750 (37.223)\t batch time: 1.30\n",
            "Epoch: [8][60/370]\tLoss 5.9517 (6.0210)\tPrec@1 30.469 (23.604)\tPrec@5 46.875 (37.590)\t batch time: 1.16\n",
            "Epoch: [8][90/370]\tLoss 5.9523 (6.0205)\tPrec@1 29.688 (23.644)\tPrec@5 42.969 (38.135)\t batch time: 1.13\n",
            "Epoch: [8][120/370]\tLoss 6.0497 (6.0232)\tPrec@1 20.312 (23.360)\tPrec@5 35.156 (38.010)\t batch time: 1.27\n",
            "Epoch: [8][150/370]\tLoss 5.9396 (6.0205)\tPrec@1 29.688 (23.593)\tPrec@5 43.750 (38.343)\t batch time: 1.18\n",
            "Epoch: [8][180/370]\tLoss 5.9932 (6.0205)\tPrec@1 25.781 (23.567)\tPrec@5 39.062 (38.204)\t batch time: 1.20\n",
            "Epoch: [8][210/370]\tLoss 6.0111 (6.0199)\tPrec@1 25.000 (23.652)\tPrec@5 36.719 (38.248)\t batch time: 1.14\n",
            "Epoch: [8][240/370]\tLoss 6.0209 (6.0198)\tPrec@1 25.781 (23.655)\tPrec@5 35.938 (38.401)\t batch time: 1.17\n",
            "Epoch: [8][270/370]\tLoss 5.9696 (6.0187)\tPrec@1 27.344 (23.740)\tPrec@5 35.938 (38.489)\t batch time: 1.23\n",
            "Epoch: [8][300/370]\tLoss 6.0103 (6.0192)\tPrec@1 22.656 (23.663)\tPrec@5 41.406 (38.486)\t batch time: 1.18\n",
            "Epoch: [8][330/370]\tLoss 6.0463 (6.0188)\tPrec@1 22.656 (23.697)\tPrec@5 34.375 (38.626)\t batch time: 1.27\n",
            "Epoch: [8][360/370]\tLoss 5.9651 (6.0181)\tPrec@1 28.906 (23.799)\tPrec@5 40.625 (38.738)\t batch time: 1.22\n",
            "Test\t  Prec@1: 18.629\t Prec@5: 35.295 (Err: 81.371 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [9][0/370]\tLoss 6.0117 (6.0117)\tPrec@1 25.781 (25.781)\tPrec@5 45.312 (45.312)\t batch time: 4.14\n",
            "Epoch: [9][30/370]\tLoss 6.0504 (6.0140)\tPrec@1 21.875 (24.420)\tPrec@5 42.188 (40.398)\t batch time: 1.22\n",
            "Epoch: [9][60/370]\tLoss 6.0027 (6.0097)\tPrec@1 27.344 (24.488)\tPrec@5 47.656 (40.100)\t batch time: 1.22\n",
            "Epoch: [9][90/370]\tLoss 5.9818 (6.0057)\tPrec@1 28.906 (24.760)\tPrec@5 43.750 (39.827)\t batch time: 1.18\n",
            "Epoch: [9][120/370]\tLoss 6.0343 (6.0038)\tPrec@1 21.875 (24.955)\tPrec@5 35.156 (39.786)\t batch time: 1.21\n",
            "Epoch: [9][150/370]\tLoss 5.9755 (6.0020)\tPrec@1 30.469 (25.026)\tPrec@5 46.875 (39.921)\t batch time: 1.21\n",
            "Epoch: [9][180/370]\tLoss 5.9790 (6.0012)\tPrec@1 27.344 (25.177)\tPrec@5 42.188 (39.991)\t batch time: 1.26\n",
            "Epoch: [9][210/370]\tLoss 5.9419 (6.0001)\tPrec@1 32.812 (25.270)\tPrec@5 44.531 (40.081)\t batch time: 1.14\n",
            "Epoch: [9][240/370]\tLoss 5.9853 (5.9993)\tPrec@1 28.906 (25.389)\tPrec@5 39.844 (40.077)\t batch time: 1.17\n",
            "Epoch: [9][270/370]\tLoss 6.0368 (6.0005)\tPrec@1 21.094 (25.291)\tPrec@5 39.062 (39.976)\t batch time: 1.14\n",
            "Epoch: [9][300/370]\tLoss 6.0142 (6.0000)\tPrec@1 24.219 (25.301)\tPrec@5 39.844 (39.989)\t batch time: 1.20\n",
            "Epoch: [9][330/370]\tLoss 6.0014 (5.9999)\tPrec@1 25.000 (25.321)\tPrec@5 40.625 (40.035)\t batch time: 1.22\n",
            "Epoch: [9][360/370]\tLoss 6.0646 (6.0005)\tPrec@1 20.312 (25.234)\tPrec@5 32.031 (39.935)\t batch time: 1.20\n",
            "Test\t  Prec@1: 25.962\t Prec@5: 40.095 (Err: 74.038 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [10][0/370]\tLoss 5.8993 (5.8993)\tPrec@1 35.156 (35.156)\tPrec@5 48.438 (48.438)\t batch time: 4.58\n",
            "Epoch: [10][30/370]\tLoss 6.0734 (5.9921)\tPrec@1 19.531 (25.832)\tPrec@5 38.281 (40.146)\t batch time: 1.23\n",
            "Epoch: [10][60/370]\tLoss 5.9815 (5.9883)\tPrec@1 27.344 (26.191)\tPrec@5 41.406 (40.817)\t batch time: 1.22\n",
            "Epoch: [10][90/370]\tLoss 6.0265 (5.9926)\tPrec@1 22.656 (25.764)\tPrec@5 41.406 (40.453)\t batch time: 1.14\n",
            "Epoch: [10][120/370]\tLoss 5.9313 (5.9859)\tPrec@1 33.594 (26.291)\tPrec@5 42.188 (40.793)\t batch time: 1.17\n",
            "Epoch: [10][150/370]\tLoss 5.8957 (5.9850)\tPrec@1 35.156 (26.350)\tPrec@5 48.438 (40.796)\t batch time: 1.25\n",
            "Epoch: [10][180/370]\tLoss 6.0300 (5.9836)\tPrec@1 20.312 (26.398)\tPrec@5 38.281 (40.910)\t batch time: 1.18\n",
            "Epoch: [10][210/370]\tLoss 6.0260 (5.9847)\tPrec@1 21.094 (26.307)\tPrec@5 39.062 (40.714)\t batch time: 1.24\n",
            "Epoch: [10][240/370]\tLoss 5.9007 (5.9841)\tPrec@1 32.031 (26.378)\tPrec@5 47.656 (40.703)\t batch time: 1.20\n",
            "Epoch: [10][270/370]\tLoss 5.9013 (5.9821)\tPrec@1 35.156 (26.560)\tPrec@5 49.219 (40.772)\t batch time: 1.28\n",
            "Epoch: [10][300/370]\tLoss 6.0040 (5.9827)\tPrec@1 25.000 (26.461)\tPrec@5 35.938 (40.721)\t batch time: 1.14\n",
            "Epoch: [10][330/370]\tLoss 5.9825 (5.9845)\tPrec@1 27.344 (26.315)\tPrec@5 41.406 (40.559)\t batch time: 1.21\n",
            "Epoch: [10][360/370]\tLoss 5.9635 (5.9841)\tPrec@1 29.688 (26.337)\tPrec@5 37.500 (40.484)\t batch time: 1.26\n",
            "Test\t  Prec@1: 13.657\t Prec@5: 30.990 (Err: 86.343 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [11][0/370]\tLoss 6.0092 (6.0092)\tPrec@1 25.781 (25.781)\tPrec@5 36.719 (36.719)\t batch time: 4.60\n",
            "Epoch: [11][30/370]\tLoss 5.9661 (5.9821)\tPrec@1 25.781 (26.512)\tPrec@5 46.875 (41.658)\t batch time: 1.17\n",
            "Epoch: [11][60/370]\tLoss 6.0961 (5.9762)\tPrec@1 17.188 (26.895)\tPrec@5 26.562 (41.317)\t batch time: 1.23\n",
            "Epoch: [11][90/370]\tLoss 6.1052 (5.9741)\tPrec@1 14.062 (27.078)\tPrec@5 31.250 (41.647)\t batch time: 1.24\n",
            "Epoch: [11][120/370]\tLoss 6.0328 (5.9719)\tPrec@1 21.875 (27.221)\tPrec@5 33.594 (41.542)\t batch time: 1.25\n",
            "Epoch: [11][150/370]\tLoss 5.9250 (5.9704)\tPrec@1 30.469 (27.323)\tPrec@5 42.969 (41.582)\t batch time: 1.17\n",
            "Epoch: [11][180/370]\tLoss 5.9450 (5.9706)\tPrec@1 31.250 (27.309)\tPrec@5 39.844 (41.518)\t batch time: 1.16\n",
            "Epoch: [11][210/370]\tLoss 5.9091 (5.9693)\tPrec@1 32.812 (27.447)\tPrec@5 45.312 (41.484)\t batch time: 1.35\n",
            "Epoch: [11][240/370]\tLoss 5.9484 (5.9699)\tPrec@1 28.906 (27.431)\tPrec@5 42.969 (41.387)\t batch time: 1.25\n",
            "Epoch: [11][270/370]\tLoss 5.9683 (5.9694)\tPrec@1 27.344 (27.448)\tPrec@5 43.750 (41.383)\t batch time: 1.23\n",
            "Epoch: [11][300/370]\tLoss 5.9732 (5.9703)\tPrec@1 26.562 (27.390)\tPrec@5 45.312 (41.238)\t batch time: 1.23\n",
            "Epoch: [11][330/370]\tLoss 5.9913 (5.9705)\tPrec@1 25.000 (27.320)\tPrec@5 42.969 (41.229)\t batch time: 1.19\n",
            "Epoch: [11][360/370]\tLoss 5.9694 (5.9692)\tPrec@1 26.562 (27.391)\tPrec@5 36.719 (41.322)\t batch time: 1.07\n",
            "Test\t  Prec@1: 15.314\t Prec@5: 31.371 (Err: 84.686 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [12][0/370]\tLoss 5.9953 (5.9953)\tPrec@1 22.656 (22.656)\tPrec@5 45.312 (45.312)\t batch time: 4.43\n",
            "Epoch: [12][30/370]\tLoss 5.9784 (5.9672)\tPrec@1 28.125 (27.419)\tPrec@5 39.844 (41.431)\t batch time: 1.17\n",
            "Epoch: [12][60/370]\tLoss 6.0368 (5.9679)\tPrec@1 22.656 (27.421)\tPrec@5 40.625 (41.419)\t batch time: 1.23\n",
            "Epoch: [12][90/370]\tLoss 5.9420 (5.9582)\tPrec@1 31.250 (28.091)\tPrec@5 41.406 (41.758)\t batch time: 1.25\n",
            "Epoch: [12][120/370]\tLoss 5.9358 (5.9541)\tPrec@1 29.688 (28.480)\tPrec@5 39.844 (41.955)\t batch time: 1.20\n",
            "Epoch: [12][150/370]\tLoss 5.9866 (5.9537)\tPrec@1 24.219 (28.508)\tPrec@5 40.625 (41.913)\t batch time: 1.19\n",
            "Epoch: [12][180/370]\tLoss 5.9654 (5.9533)\tPrec@1 27.344 (28.475)\tPrec@5 39.844 (41.803)\t batch time: 1.29\n",
            "Epoch: [12][210/370]\tLoss 5.9844 (5.9548)\tPrec@1 25.781 (28.380)\tPrec@5 39.062 (41.636)\t batch time: 1.21\n",
            "Epoch: [12][240/370]\tLoss 5.9681 (5.9538)\tPrec@1 28.125 (28.508)\tPrec@5 49.219 (41.763)\t batch time: 1.15\n",
            "Epoch: [12][270/370]\tLoss 5.9336 (5.9538)\tPrec@1 30.469 (28.520)\tPrec@5 44.531 (41.798)\t batch time: 1.18\n",
            "Epoch: [12][300/370]\tLoss 6.0189 (5.9541)\tPrec@1 22.656 (28.423)\tPrec@5 35.938 (41.718)\t batch time: 1.20\n",
            "Epoch: [12][330/370]\tLoss 5.9927 (5.9543)\tPrec@1 25.000 (28.399)\tPrec@5 39.844 (41.715)\t batch time: 1.23\n",
            "Epoch: [12][360/370]\tLoss 5.9249 (5.9529)\tPrec@1 31.250 (28.528)\tPrec@5 47.656 (41.753)\t batch time: 1.22\n",
            "Test\t  Prec@1: 25.295\t Prec@5: 41.048 (Err: 74.705 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [13][0/370]\tLoss 5.9024 (5.9024)\tPrec@1 31.250 (31.250)\tPrec@5 44.531 (44.531)\t batch time: 3.97\n",
            "Epoch: [13][30/370]\tLoss 5.9064 (5.9525)\tPrec@1 30.469 (28.352)\tPrec@5 51.562 (42.994)\t batch time: 1.20\n",
            "Epoch: [13][60/370]\tLoss 5.9541 (5.9532)\tPrec@1 28.125 (28.266)\tPrec@5 41.406 (42.456)\t batch time: 1.14\n",
            "Epoch: [13][90/370]\tLoss 5.9462 (5.9514)\tPrec@1 29.688 (28.546)\tPrec@5 41.406 (42.548)\t batch time: 1.14\n",
            "Epoch: [13][120/370]\tLoss 5.9931 (5.9508)\tPrec@1 25.781 (28.616)\tPrec@5 45.312 (42.323)\t batch time: 1.17\n",
            "Epoch: [13][150/370]\tLoss 5.9651 (5.9516)\tPrec@1 28.125 (28.601)\tPrec@5 45.312 (42.213)\t batch time: 1.14\n",
            "Epoch: [13][180/370]\tLoss 5.9530 (5.9510)\tPrec@1 27.344 (28.626)\tPrec@5 39.844 (42.222)\t batch time: 1.25\n",
            "Epoch: [13][210/370]\tLoss 5.9583 (5.9504)\tPrec@1 28.906 (28.595)\tPrec@5 39.062 (42.273)\t batch time: 1.25\n",
            "Epoch: [13][240/370]\tLoss 5.9455 (5.9507)\tPrec@1 29.688 (28.508)\tPrec@5 41.406 (42.197)\t batch time: 1.17\n",
            "Epoch: [13][270/370]\tLoss 5.9559 (5.9484)\tPrec@1 28.906 (28.716)\tPrec@5 44.531 (42.352)\t batch time: 1.16\n",
            "Epoch: [13][300/370]\tLoss 5.9329 (5.9477)\tPrec@1 29.688 (28.774)\tPrec@5 39.062 (42.424)\t batch time: 1.19\n",
            "Epoch: [13][330/370]\tLoss 5.9282 (5.9472)\tPrec@1 29.688 (28.769)\tPrec@5 41.406 (42.459)\t batch time: 1.21\n",
            "Epoch: [13][360/370]\tLoss 5.9299 (5.9462)\tPrec@1 30.469 (28.802)\tPrec@5 38.281 (42.445)\t batch time: 1.25\n",
            "Test\t  Prec@1: 25.790\t Prec@5: 40.648 (Err: 74.210 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [14][0/370]\tLoss 5.9944 (5.9944)\tPrec@1 24.219 (24.219)\tPrec@5 44.531 (44.531)\t batch time: 4.12\n",
            "Epoch: [14][30/370]\tLoss 5.9313 (5.9597)\tPrec@1 30.469 (27.898)\tPrec@5 42.188 (42.087)\t batch time: 1.23\n",
            "Epoch: [14][60/370]\tLoss 5.9631 (5.9569)\tPrec@1 28.906 (28.087)\tPrec@5 33.594 (41.445)\t batch time: 1.21\n",
            "Epoch: [14][90/370]\tLoss 5.9632 (5.9518)\tPrec@1 26.562 (28.511)\tPrec@5 39.062 (41.690)\t batch time: 1.25\n",
            "Epoch: [14][120/370]\tLoss 5.9589 (5.9470)\tPrec@1 28.906 (28.958)\tPrec@5 42.969 (42.065)\t batch time: 1.24\n",
            "Epoch: [14][150/370]\tLoss 6.0247 (5.9451)\tPrec@1 22.656 (29.139)\tPrec@5 39.062 (42.534)\t batch time: 1.12\n",
            "Epoch: [14][180/370]\tLoss 5.8755 (5.9416)\tPrec@1 36.719 (29.437)\tPrec@5 48.438 (42.904)\t batch time: 1.20\n",
            "Epoch: [14][210/370]\tLoss 5.9266 (5.9395)\tPrec@1 31.250 (29.576)\tPrec@5 42.188 (42.984)\t batch time: 1.15\n",
            "Epoch: [14][240/370]\tLoss 5.9594 (5.9388)\tPrec@1 29.688 (29.584)\tPrec@5 37.500 (42.917)\t batch time: 1.18\n",
            "Epoch: [14][270/370]\tLoss 5.9424 (5.9372)\tPrec@1 28.906 (29.711)\tPrec@5 42.188 (42.980)\t batch time: 1.24\n",
            "Epoch: [14][300/370]\tLoss 5.9603 (5.9368)\tPrec@1 26.562 (29.688)\tPrec@5 39.062 (42.917)\t batch time: 1.26\n",
            "Epoch: [14][330/370]\tLoss 5.9067 (5.9384)\tPrec@1 32.031 (29.508)\tPrec@5 46.875 (42.839)\t batch time: 1.21\n",
            "Epoch: [14][360/370]\tLoss 5.9844 (5.9393)\tPrec@1 28.125 (29.449)\tPrec@5 41.406 (42.852)\t batch time: 1.12\n",
            "Test\t  Prec@1: 27.600\t Prec@5: 42.476 (Err: 72.400 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [15][0/370]\tLoss 5.9041 (5.9041)\tPrec@1 33.594 (33.594)\tPrec@5 47.656 (47.656)\t batch time: 4.85\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [15][30/370]\tLoss 5.9078 (5.9473)\tPrec@1 28.125 (28.125)\tPrec@5 43.750 (41.658)\t batch time: 1.33\n",
            "Epoch: [15][60/370]\tLoss 5.9808 (5.9432)\tPrec@1 25.781 (28.663)\tPrec@5 39.062 (42.328)\t batch time: 1.23\n",
            "Epoch: [15][90/370]\tLoss 5.9238 (5.9410)\tPrec@1 29.688 (29.121)\tPrec@5 42.969 (42.874)\t batch time: 1.21\n",
            "Epoch: [15][120/370]\tLoss 5.9375 (5.9413)\tPrec@1 28.906 (29.300)\tPrec@5 43.750 (42.885)\t batch time: 1.22\n",
            "Epoch: [15][150/370]\tLoss 5.9106 (5.9408)\tPrec@1 33.594 (29.294)\tPrec@5 42.188 (43.010)\t batch time: 1.31\n",
            "Epoch: [15][180/370]\tLoss 5.8789 (5.9376)\tPrec@1 35.938 (29.593)\tPrec@5 48.438 (43.288)\t batch time: 1.17\n",
            "Epoch: [15][210/370]\tLoss 5.9797 (5.9338)\tPrec@1 24.219 (29.884)\tPrec@5 40.625 (43.561)\t batch time: 1.20\n",
            "Epoch: [15][240/370]\tLoss 5.9429 (5.9329)\tPrec@1 30.469 (29.921)\tPrec@5 46.875 (43.666)\t batch time: 1.24\n",
            "Epoch: [15][270/370]\tLoss 5.9351 (5.9335)\tPrec@1 29.688 (29.909)\tPrec@5 44.531 (43.623)\t batch time: 1.24\n",
            "Epoch: [15][300/370]\tLoss 5.9830 (5.9313)\tPrec@1 25.781 (30.074)\tPrec@5 39.844 (43.708)\t batch time: 1.14\n",
            "Epoch: [15][330/370]\tLoss 5.9143 (5.9305)\tPrec@1 33.594 (30.122)\tPrec@5 48.438 (43.731)\t batch time: 1.24\n",
            "Epoch: [15][360/370]\tLoss 5.9827 (5.9317)\tPrec@1 28.125 (30.029)\tPrec@5 39.062 (43.657)\t batch time: 1.21\n",
            "Test\t  Prec@1: 20.552\t Prec@5: 36.914 (Err: 79.448 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [16][0/370]\tLoss 6.0356 (6.0356)\tPrec@1 22.656 (22.656)\tPrec@5 36.719 (36.719)\t batch time: 4.37\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [16][30/370]\tLoss 5.8842 (5.9133)\tPrec@1 36.719 (31.628)\tPrec@5 53.125 (44.153)\t batch time: 1.13\n",
            "Epoch: [16][60/370]\tLoss 5.9243 (5.9132)\tPrec@1 29.688 (31.711)\tPrec@5 40.625 (44.365)\t batch time: 1.25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [16][90/370]\tLoss 6.0062 (5.9194)\tPrec@1 27.344 (31.130)\tPrec@5 38.281 (43.879)\t batch time: 1.19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [16][120/370]\tLoss 5.8925 (5.9173)\tPrec@1 31.250 (31.327)\tPrec@5 42.188 (44.208)\t batch time: 1.26\n",
            "Epoch: [16][150/370]\tLoss 5.9367 (5.9183)\tPrec@1 31.250 (31.234)\tPrec@5 42.969 (44.019)\t batch time: 1.09\n",
            "Epoch: [16][180/370]\tLoss 5.9353 (5.9204)\tPrec@1 32.812 (31.086)\tPrec@5 45.312 (43.979)\t batch time: 1.20\n",
            "Epoch: [16][210/370]\tLoss 5.9200 (5.9192)\tPrec@1 32.031 (31.157)\tPrec@5 42.969 (44.072)\t batch time: 1.29\n",
            "Epoch: [16][240/370]\tLoss 5.9882 (5.9166)\tPrec@1 26.562 (31.402)\tPrec@5 39.844 (44.308)\t batch time: 1.12\n",
            "Epoch: [16][270/370]\tLoss 5.9145 (5.9155)\tPrec@1 31.250 (31.484)\tPrec@5 37.500 (44.329)\t batch time: 1.18\n",
            "Epoch: [16][300/370]\tLoss 5.8715 (5.9158)\tPrec@1 33.594 (31.543)\tPrec@5 42.969 (44.378)\t batch time: 1.13\n",
            "Epoch: [16][330/370]\tLoss 5.9130 (5.9162)\tPrec@1 32.031 (31.578)\tPrec@5 47.656 (44.390)\t batch time: 1.13\n",
            "Epoch: [16][360/370]\tLoss 5.9088 (5.9164)\tPrec@1 31.250 (31.551)\tPrec@5 45.312 (44.391)\t batch time: 1.17\n",
            "Test\t  Prec@1: 31.543\t Prec@5: 43.924 (Err: 68.457 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-02\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [17][0/370]\tLoss 5.9047 (5.9047)\tPrec@1 32.031 (32.031)\tPrec@5 50.781 (50.781)\t batch time: 4.96\n",
            "Epoch: [17][30/370]\tLoss 5.9471 (5.9197)\tPrec@1 32.812 (31.678)\tPrec@5 42.969 (43.800)\t batch time: 1.24\n",
            "Epoch: [17][60/370]\tLoss 6.0385 (5.9153)\tPrec@1 24.219 (32.134)\tPrec@5 39.062 (44.339)\t batch time: 1.25\n",
            "Epoch: [17][90/370]\tLoss 5.9284 (5.9109)\tPrec@1 29.688 (32.461)\tPrec@5 40.625 (44.428)\t batch time: 1.23\n",
            "Epoch: [17][120/370]\tLoss 5.8173 (5.9095)\tPrec@1 38.281 (32.606)\tPrec@5 53.906 (44.680)\t batch time: 1.16\n",
            "Epoch: [17][150/370]\tLoss 5.8555 (5.9119)\tPrec@1 37.500 (32.383)\tPrec@5 46.875 (44.567)\t batch time: 1.24\n",
            "Epoch: [17][180/370]\tLoss 5.9544 (5.9151)\tPrec@1 30.469 (32.092)\tPrec@5 46.094 (44.432)\t batch time: 1.21\n",
            "Epoch: [17][210/370]\tLoss 5.8406 (5.9122)\tPrec@1 38.281 (32.313)\tPrec@5 52.344 (44.546)\t batch time: 1.20\n",
            "Epoch: [17][240/370]\tLoss 5.8868 (5.9091)\tPrec@1 34.375 (32.563)\tPrec@5 46.875 (44.616)\t batch time: 1.20\n",
            "Epoch: [17][270/370]\tLoss 5.9195 (5.9084)\tPrec@1 32.031 (32.619)\tPrec@5 41.406 (44.589)\t batch time: 1.25\n",
            "Epoch: [17][300/370]\tLoss 5.9094 (5.9074)\tPrec@1 32.031 (32.667)\tPrec@5 44.531 (44.645)\t batch time: 1.24\n",
            "Epoch: [17][330/370]\tLoss 5.8430 (5.9055)\tPrec@1 38.281 (32.836)\tPrec@5 51.562 (44.760)\t batch time: 1.34\n",
            "Epoch: [17][360/370]\tLoss 5.8624 (5.9052)\tPrec@1 38.281 (32.873)\tPrec@5 42.969 (44.722)\t batch time: 1.18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test\t  Prec@1: 29.543\t Prec@5: 42.514 (Err: 70.457 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "    self._shutdown_workers()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    self._shutdown_workers()\n",
            "    w.join()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [18][0/370]\tLoss 5.8889 (5.8889)\tPrec@1 32.812 (32.812)\tPrec@5 46.094 (46.094)\t batch time: 4.98\n",
            "Epoch: [18][30/370]\tLoss 5.9201 (5.8964)\tPrec@1 35.156 (33.392)\tPrec@5 40.625 (44.153)\t batch time: 1.16\n",
            "Epoch: [18][60/370]\tLoss 5.8519 (5.8964)\tPrec@1 41.406 (33.517)\tPrec@5 50.781 (44.173)\t batch time: 1.14\n",
            "Epoch: [18][90/370]\tLoss 5.8305 (5.8927)\tPrec@1 39.844 (33.980)\tPrec@5 49.219 (44.428)\t batch time: 1.21\n",
            "Epoch: [18][120/370]\tLoss 5.9298 (5.8873)\tPrec@1 31.250 (34.472)\tPrec@5 40.625 (44.699)\t batch time: 1.14\n",
            "Epoch: [18][150/370]\tLoss 5.8669 (5.8819)\tPrec@1 36.719 (34.908)\tPrec@5 42.188 (44.961)\t batch time: 1.16\n",
            "Epoch: [18][180/370]\tLoss 5.7940 (5.8782)\tPrec@1 40.625 (35.204)\tPrec@5 53.906 (45.153)\t batch time: 1.18\n",
            "Epoch: [18][210/370]\tLoss 5.7575 (5.8766)\tPrec@1 46.094 (35.323)\tPrec@5 54.688 (45.157)\t batch time: 1.16\n",
            "Epoch: [18][240/370]\tLoss 5.8315 (5.8758)\tPrec@1 39.062 (35.425)\tPrec@5 43.750 (45.261)\t batch time: 1.14\n",
            "Epoch: [18][270/370]\tLoss 5.8379 (5.8738)\tPrec@1 39.844 (35.586)\tPrec@5 47.656 (45.362)\t batch time: 1.26\n",
            "Epoch: [18][300/370]\tLoss 5.8176 (5.8721)\tPrec@1 40.625 (35.738)\tPrec@5 49.219 (45.479)\t batch time: 1.13\n",
            "Epoch: [18][330/370]\tLoss 5.9227 (5.8718)\tPrec@1 32.812 (35.819)\tPrec@5 46.094 (45.537)\t batch time: 1.29\n",
            "Epoch: [18][360/370]\tLoss 5.9157 (5.8712)\tPrec@1 35.156 (35.853)\tPrec@5 44.531 (45.572)\t batch time: 1.32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "    self._shutdown_workers()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1341d13208>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test\t  Prec@1: 38.286\t Prec@5: 46.476 (Err: 61.714 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [19][0/370]\tLoss 5.8721 (5.8721)\tPrec@1 37.500 (37.500)\tPrec@5 48.438 (48.438)\t batch time: 4.54\n",
            "Epoch: [19][30/370]\tLoss 5.7876 (5.8499)\tPrec@1 40.625 (38.029)\tPrec@5 50.000 (46.648)\t batch time: 1.25\n",
            "Epoch: [19][60/370]\tLoss 5.8920 (5.8559)\tPrec@1 33.594 (37.154)\tPrec@5 41.406 (46.094)\t batch time: 1.16\n",
            "Epoch: [19][90/370]\tLoss 5.9130 (5.8547)\tPrec@1 35.938 (37.337)\tPrec@5 42.969 (46.120)\t batch time: 1.18\n",
            "Epoch: [19][120/370]\tLoss 5.9068 (5.8526)\tPrec@1 34.375 (37.468)\tPrec@5 46.094 (46.365)\t batch time: 1.14\n",
            "Epoch: [19][150/370]\tLoss 5.9020 (5.8566)\tPrec@1 33.594 (37.200)\tPrec@5 42.188 (46.083)\t batch time: 1.15\n",
            "Epoch: [19][180/370]\tLoss 5.8305 (5.8611)\tPrec@1 38.281 (36.878)\tPrec@5 48.438 (45.822)\t batch time: 1.16\n",
            "Epoch: [19][210/370]\tLoss 5.8104 (5.8609)\tPrec@1 42.188 (36.856)\tPrec@5 49.219 (45.831)\t batch time: 1.31\n",
            "Epoch: [19][240/370]\tLoss 5.8779 (5.8608)\tPrec@1 32.812 (36.848)\tPrec@5 43.750 (45.763)\t batch time: 1.21\n",
            "Epoch: [19][270/370]\tLoss 5.8208 (5.8601)\tPrec@1 40.625 (36.918)\tPrec@5 46.875 (45.811)\t batch time: 1.24\n",
            "Epoch: [19][300/370]\tLoss 5.8704 (5.8597)\tPrec@1 37.500 (36.934)\tPrec@5 46.875 (45.837)\t batch time: 1.25\n",
            "Epoch: [19][330/370]\tLoss 5.9247 (5.8596)\tPrec@1 32.812 (36.908)\tPrec@5 40.625 (45.822)\t batch time: 1.35\n",
            "Epoch: [19][360/370]\tLoss 5.8973 (5.8602)\tPrec@1 33.594 (36.875)\tPrec@5 41.406 (45.838)\t batch time: 1.18\n",
            "Test\t  Prec@1: 38.610\t Prec@5: 46.457 (Err: 61.390 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [20][0/370]\tLoss 5.7856 (5.7856)\tPrec@1 39.844 (39.844)\tPrec@5 48.438 (48.438)\t batch time: 4.59\n",
            "Epoch: [20][30/370]\tLoss 5.8821 (5.8548)\tPrec@1 35.938 (37.122)\tPrec@5 43.750 (45.464)\t batch time: 1.25\n",
            "Epoch: [20][60/370]\tLoss 5.8667 (5.8542)\tPrec@1 34.375 (37.538)\tPrec@5 43.750 (45.966)\t batch time: 1.28\n",
            "Epoch: [20][90/370]\tLoss 5.8846 (5.8531)\tPrec@1 35.938 (37.534)\tPrec@5 42.188 (46.197)\t batch time: 1.21\n",
            "Epoch: [20][120/370]\tLoss 5.7177 (5.8527)\tPrec@1 46.875 (37.610)\tPrec@5 57.812 (46.404)\t batch time: 1.13\n",
            "Epoch: [20][150/370]\tLoss 5.9194 (5.8547)\tPrec@1 32.812 (37.474)\tPrec@5 44.531 (46.223)\t batch time: 1.24\n",
            "Epoch: [20][180/370]\tLoss 5.8400 (5.8539)\tPrec@1 36.719 (37.465)\tPrec@5 45.312 (46.202)\t batch time: 1.19\n",
            "Epoch: [20][210/370]\tLoss 5.8611 (5.8545)\tPrec@1 37.500 (37.493)\tPrec@5 49.219 (46.138)\t batch time: 1.31\n",
            "Epoch: [20][240/370]\tLoss 5.8408 (5.8553)\tPrec@1 38.281 (37.442)\tPrec@5 46.875 (46.136)\t batch time: 1.17\n",
            "Epoch: [20][270/370]\tLoss 5.9144 (5.8547)\tPrec@1 32.031 (37.434)\tPrec@5 42.969 (46.140)\t batch time: 1.25\n",
            "Epoch: [20][300/370]\tLoss 5.8967 (5.8545)\tPrec@1 35.938 (37.479)\tPrec@5 45.312 (46.164)\t batch time: 1.16\n",
            "Epoch: [20][330/370]\tLoss 5.8925 (5.8561)\tPrec@1 32.031 (37.337)\tPrec@5 43.750 (46.028)\t batch time: 1.17\n",
            "Epoch: [20][360/370]\tLoss 5.9102 (5.8551)\tPrec@1 32.812 (37.387)\tPrec@5 39.844 (46.031)\t batch time: 1.12\n",
            "Test\t  Prec@1: 38.800\t Prec@5: 46.476 (Err: 61.200 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [21][0/370]\tLoss 5.9677 (5.9677)\tPrec@1 27.344 (27.344)\tPrec@5 39.062 (39.062)\t batch time: 4.85\n",
            "Epoch: [21][30/370]\tLoss 5.8558 (5.8607)\tPrec@1 39.062 (36.946)\tPrec@5 48.438 (45.514)\t batch time: 1.19\n",
            "Epoch: [21][60/370]\tLoss 5.8619 (5.8573)\tPrec@1 37.500 (37.257)\tPrec@5 43.750 (45.863)\t batch time: 1.14\n",
            "Epoch: [21][90/370]\tLoss 5.9093 (5.8591)\tPrec@1 31.250 (37.028)\tPrec@5 40.625 (45.802)\t batch time: 1.35\n",
            "Epoch: [21][120/370]\tLoss 5.8785 (5.8554)\tPrec@1 35.156 (37.300)\tPrec@5 44.531 (46.036)\t batch time: 1.15\n",
            "Epoch: [21][150/370]\tLoss 5.9436 (5.8571)\tPrec@1 27.344 (37.210)\tPrec@5 36.719 (45.944)\t batch time: 1.20\n",
            "Epoch: [21][180/370]\tLoss 5.8508 (5.8548)\tPrec@1 35.156 (37.444)\tPrec@5 42.188 (46.012)\t batch time: 1.23\n",
            "Epoch: [21][210/370]\tLoss 5.8015 (5.8532)\tPrec@1 38.281 (37.530)\tPrec@5 48.438 (46.157)\t batch time: 1.41\n",
            "Epoch: [21][240/370]\tLoss 5.8912 (5.8541)\tPrec@1 36.719 (37.481)\tPrec@5 40.625 (45.993)\t batch time: 1.15\n",
            "Epoch: [21][270/370]\tLoss 5.8780 (5.8544)\tPrec@1 37.500 (37.457)\tPrec@5 48.438 (45.961)\t batch time: 1.25\n",
            "Epoch: [21][300/370]\tLoss 5.8291 (5.8539)\tPrec@1 39.062 (37.518)\tPrec@5 49.219 (45.948)\t batch time: 1.13\n",
            "Epoch: [21][330/370]\tLoss 5.8807 (5.8521)\tPrec@1 37.500 (37.677)\tPrec@5 43.750 (46.155)\t batch time: 1.21\n",
            "Epoch: [21][360/370]\tLoss 5.7850 (5.8529)\tPrec@1 42.969 (37.565)\tPrec@5 53.906 (46.001)\t batch time: 1.29\n",
            "Test\t  Prec@1: 39.333\t Prec@5: 46.914 (Err: 60.667 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [22][0/370]\tLoss 5.7385 (5.7385)\tPrec@1 46.875 (46.875)\tPrec@5 54.688 (54.688)\t batch time: 4.57\n",
            "Epoch: [22][30/370]\tLoss 5.8749 (5.8436)\tPrec@1 38.281 (38.306)\tPrec@5 46.094 (46.396)\t batch time: 1.19\n",
            "Epoch: [22][60/370]\tLoss 5.9427 (5.8523)\tPrec@1 29.688 (37.846)\tPrec@5 40.625 (45.966)\t batch time: 1.16\n",
            "Epoch: [22][90/370]\tLoss 5.8812 (5.8555)\tPrec@1 32.812 (37.560)\tPrec@5 44.531 (45.853)\t batch time: 1.23\n",
            "Epoch: [22][120/370]\tLoss 5.8816 (5.8550)\tPrec@1 35.938 (37.597)\tPrec@5 38.281 (45.945)\t batch time: 1.19\n",
            "Epoch: [22][150/370]\tLoss 5.8934 (5.8540)\tPrec@1 36.719 (37.645)\tPrec@5 44.531 (46.006)\t batch time: 1.24\n",
            "Epoch: [22][180/370]\tLoss 5.7935 (5.8541)\tPrec@1 41.406 (37.625)\tPrec@5 53.125 (45.861)\t batch time: 1.29\n",
            "Epoch: [22][210/370]\tLoss 5.7519 (5.8523)\tPrec@1 46.094 (37.781)\tPrec@5 53.125 (45.909)\t batch time: 1.20\n",
            "Epoch: [22][240/370]\tLoss 5.8252 (5.8518)\tPrec@1 39.844 (37.814)\tPrec@5 47.656 (46.019)\t batch time: 1.18\n",
            "Epoch: [22][270/370]\tLoss 5.8634 (5.8514)\tPrec@1 39.062 (37.860)\tPrec@5 50.781 (46.065)\t batch time: 1.13\n",
            "Epoch: [22][300/370]\tLoss 5.8959 (5.8503)\tPrec@1 33.594 (37.897)\tPrec@5 43.750 (46.073)\t batch time: 1.21\n",
            "Epoch: [22][330/370]\tLoss 5.7940 (5.8496)\tPrec@1 37.500 (37.946)\tPrec@5 50.000 (46.167)\t batch time: 1.19\n",
            "Epoch: [22][360/370]\tLoss 5.7066 (5.8503)\tPrec@1 49.219 (37.868)\tPrec@5 57.812 (46.068)\t batch time: 1.22\n",
            "Test\t  Prec@1: 38.895\t Prec@5: 46.362 (Err: 61.105 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [23][0/370]\tLoss 5.8518 (5.8518)\tPrec@1 35.938 (35.938)\tPrec@5 43.750 (43.750)\t batch time: 4.45\n",
            "Epoch: [23][30/370]\tLoss 5.8315 (5.8517)\tPrec@1 42.188 (37.802)\tPrec@5 47.656 (45.161)\t batch time: 1.18\n",
            "Epoch: [23][60/370]\tLoss 5.7587 (5.8517)\tPrec@1 44.531 (37.833)\tPrec@5 49.219 (45.402)\t batch time: 1.17\n",
            "Epoch: [23][90/370]\tLoss 5.8263 (5.8482)\tPrec@1 39.844 (37.998)\tPrec@5 44.531 (45.622)\t batch time: 1.22\n",
            "Epoch: [23][120/370]\tLoss 5.8457 (5.8460)\tPrec@1 36.719 (38.210)\tPrec@5 50.000 (46.094)\t batch time: 1.20\n",
            "Epoch: [23][150/370]\tLoss 5.7085 (5.8456)\tPrec@1 51.562 (38.240)\tPrec@5 58.594 (45.995)\t batch time: 1.27\n",
            "Epoch: [23][180/370]\tLoss 5.7788 (5.8423)\tPrec@1 42.188 (38.463)\tPrec@5 49.219 (46.361)\t batch time: 1.18\n",
            "Epoch: [23][210/370]\tLoss 5.7750 (5.8400)\tPrec@1 40.625 (38.626)\tPrec@5 49.219 (46.527)\t batch time: 1.15\n",
            "Epoch: [23][240/370]\tLoss 5.9626 (5.8413)\tPrec@1 29.688 (38.544)\tPrec@5 37.500 (46.463)\t batch time: 1.31\n",
            "Epoch: [23][270/370]\tLoss 5.8915 (5.8445)\tPrec@1 35.156 (38.313)\tPrec@5 42.188 (46.252)\t batch time: 1.13\n",
            "Epoch: [23][300/370]\tLoss 5.9643 (5.8467)\tPrec@1 28.125 (38.151)\tPrec@5 38.281 (46.161)\t batch time: 1.19\n",
            "Epoch: [23][330/370]\tLoss 5.8639 (5.8461)\tPrec@1 35.938 (38.239)\tPrec@5 44.531 (46.224)\t batch time: 1.20\n",
            "Epoch: [23][360/370]\tLoss 5.8770 (5.8458)\tPrec@1 35.938 (38.279)\tPrec@5 46.094 (46.234)\t batch time: 1.24\n",
            "Test\t  Prec@1: 39.219\t Prec@5: 46.571 (Err: 60.781 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [24][0/370]\tLoss 5.8985 (5.8985)\tPrec@1 32.812 (32.812)\tPrec@5 37.500 (37.500)\t batch time: 4.30\n",
            "Epoch: [24][30/370]\tLoss 5.7652 (5.8599)\tPrec@1 42.969 (36.794)\tPrec@5 51.562 (45.060)\t batch time: 1.23\n",
            "Epoch: [24][60/370]\tLoss 5.9114 (5.8520)\tPrec@1 32.031 (37.666)\tPrec@5 39.062 (45.543)\t batch time: 1.28\n",
            "Epoch: [24][90/370]\tLoss 5.8978 (5.8537)\tPrec@1 32.812 (37.569)\tPrec@5 40.625 (45.682)\t batch time: 1.16\n",
            "Epoch: [24][120/370]\tLoss 5.8475 (5.8519)\tPrec@1 38.281 (37.642)\tPrec@5 43.750 (45.681)\t batch time: 1.16\n",
            "Epoch: [24][150/370]\tLoss 5.8683 (5.8507)\tPrec@1 39.062 (37.790)\tPrec@5 47.656 (45.783)\t batch time: 1.13\n",
            "Epoch: [24][180/370]\tLoss 5.8607 (5.8509)\tPrec@1 35.938 (37.841)\tPrec@5 46.094 (45.856)\t batch time: 1.20\n",
            "Epoch: [24][210/370]\tLoss 5.8327 (5.8493)\tPrec@1 38.281 (37.926)\tPrec@5 46.094 (45.983)\t batch time: 1.17\n",
            "Epoch: [24][240/370]\tLoss 5.7737 (5.8506)\tPrec@1 45.312 (37.840)\tPrec@5 53.125 (45.958)\t batch time: 1.18\n",
            "Epoch: [24][270/370]\tLoss 5.8659 (5.8511)\tPrec@1 34.375 (37.806)\tPrec@5 42.969 (45.944)\t batch time: 1.29\n",
            "Epoch: [24][300/370]\tLoss 5.8130 (5.8494)\tPrec@1 42.969 (37.939)\tPrec@5 50.781 (46.099)\t batch time: 1.17\n",
            "Epoch: [24][330/370]\tLoss 5.7617 (5.8487)\tPrec@1 48.438 (37.998)\tPrec@5 51.562 (46.101)\t batch time: 1.22\n",
            "Epoch: [24][360/370]\tLoss 5.7672 (5.8465)\tPrec@1 46.094 (38.160)\tPrec@5 54.688 (46.189)\t batch time: 1.27\n",
            "Test\t  Prec@1: 39.981\t Prec@5: 46.914 (Err: 60.019 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [25][0/370]\tLoss 5.8755 (5.8755)\tPrec@1 35.156 (35.156)\tPrec@5 41.406 (41.406)\t batch time: 4.21\n",
            "Epoch: [25][30/370]\tLoss 5.8652 (5.8382)\tPrec@1 34.375 (38.785)\tPrec@5 42.969 (46.774)\t batch time: 1.15\n",
            "Epoch: [25][60/370]\tLoss 5.8530 (5.8348)\tPrec@1 39.062 (39.114)\tPrec@5 48.438 (47.093)\t batch time: 1.22\n",
            "Epoch: [25][90/370]\tLoss 5.8795 (5.8356)\tPrec@1 33.594 (39.088)\tPrec@5 42.969 (46.918)\t batch time: 1.24\n",
            "Epoch: [25][120/370]\tLoss 5.9278 (5.8355)\tPrec@1 31.250 (39.153)\tPrec@5 39.844 (46.927)\t batch time: 1.18\n",
            "Epoch: [25][150/370]\tLoss 5.8698 (5.8387)\tPrec@1 36.719 (38.897)\tPrec@5 46.094 (46.699)\t batch time: 1.23\n",
            "Epoch: [25][180/370]\tLoss 5.7834 (5.8380)\tPrec@1 44.531 (38.968)\tPrec@5 53.906 (46.603)\t batch time: 1.21\n",
            "Epoch: [25][210/370]\tLoss 5.8968 (5.8403)\tPrec@1 32.031 (38.818)\tPrec@5 43.750 (46.453)\t batch time: 1.23\n",
            "Epoch: [25][240/370]\tLoss 5.8839 (5.8392)\tPrec@1 35.938 (38.884)\tPrec@5 44.531 (46.505)\t batch time: 1.18\n",
            "Epoch: [25][270/370]\tLoss 5.8343 (5.8409)\tPrec@1 41.406 (38.725)\tPrec@5 44.531 (46.368)\t batch time: 1.21\n",
            "Epoch: [25][300/370]\tLoss 5.8176 (5.8411)\tPrec@1 39.062 (38.699)\tPrec@5 45.312 (46.340)\t batch time: 1.25\n",
            "Epoch: [25][330/370]\tLoss 5.7686 (5.8416)\tPrec@1 45.312 (38.690)\tPrec@5 49.219 (46.349)\t batch time: 1.17\n",
            "Epoch: [25][360/370]\tLoss 5.8175 (5.8411)\tPrec@1 42.188 (38.712)\tPrec@5 49.219 (46.386)\t batch time: 1.28\n",
            "Test\t  Prec@1: 40.152\t Prec@5: 47.181 (Err: 59.848 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [26][0/370]\tLoss 5.8240 (5.8240)\tPrec@1 39.844 (39.844)\tPrec@5 48.438 (48.438)\t batch time: 4.68\n",
            "Epoch: [26][30/370]\tLoss 5.8607 (5.8505)\tPrec@1 38.281 (38.029)\tPrec@5 46.094 (45.539)\t batch time: 1.26\n",
            "Epoch: [26][60/370]\tLoss 5.8272 (5.8442)\tPrec@1 39.844 (38.794)\tPrec@5 49.219 (46.068)\t batch time: 1.34\n",
            "Epoch: [26][90/370]\tLoss 5.8523 (5.8427)\tPrec@1 38.281 (38.942)\tPrec@5 43.750 (46.214)\t batch time: 1.15\n",
            "Epoch: [26][120/370]\tLoss 5.8495 (5.8428)\tPrec@1 38.281 (38.849)\tPrec@5 44.531 (46.171)\t batch time: 1.28\n",
            "Epoch: [26][150/370]\tLoss 5.9075 (5.8422)\tPrec@1 34.375 (38.742)\tPrec@5 40.625 (46.280)\t batch time: 1.24\n",
            "Epoch: [26][180/370]\tLoss 5.9121 (5.8446)\tPrec@1 32.812 (38.480)\tPrec@5 42.969 (46.150)\t batch time: 1.23\n",
            "Epoch: [26][210/370]\tLoss 5.8392 (5.8431)\tPrec@1 40.625 (38.544)\tPrec@5 50.000 (46.290)\t batch time: 1.25\n",
            "Epoch: [26][240/370]\tLoss 5.8646 (5.8432)\tPrec@1 38.281 (38.554)\tPrec@5 45.312 (46.275)\t batch time: 1.18\n",
            "Epoch: [26][270/370]\tLoss 5.8586 (5.8422)\tPrec@1 35.156 (38.613)\tPrec@5 42.969 (46.336)\t batch time: 1.20\n",
            "Epoch: [26][300/370]\tLoss 5.8857 (5.8414)\tPrec@1 32.812 (38.720)\tPrec@5 41.406 (46.405)\t batch time: 1.18\n",
            "Epoch: [26][330/370]\tLoss 5.8556 (5.8423)\tPrec@1 36.719 (38.616)\tPrec@5 49.219 (46.309)\t batch time: 1.25\n",
            "Epoch: [26][360/370]\tLoss 5.8058 (5.8404)\tPrec@1 39.062 (38.703)\tPrec@5 48.438 (46.455)\t batch time: 1.29\n",
            "Test\t  Prec@1: 39.657\t Prec@5: 46.743 (Err: 60.343 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [27][0/370]\tLoss 5.8014 (5.8014)\tPrec@1 41.406 (41.406)\tPrec@5 50.000 (50.000)\t batch time: 4.55\n",
            "Epoch: [27][30/370]\tLoss 5.8871 (5.8430)\tPrec@1 32.812 (38.155)\tPrec@5 43.750 (46.144)\t batch time: 1.23\n",
            "Epoch: [27][60/370]\tLoss 5.8316 (5.8381)\tPrec@1 41.406 (38.614)\tPrec@5 47.656 (46.350)\t batch time: 1.23\n",
            "Epoch: [27][90/370]\tLoss 5.8469 (5.8323)\tPrec@1 37.500 (39.148)\tPrec@5 47.656 (46.703)\t batch time: 1.18\n",
            "Epoch: [27][120/370]\tLoss 5.8058 (5.8333)\tPrec@1 40.625 (39.024)\tPrec@5 46.875 (46.668)\t batch time: 1.16\n",
            "Epoch: [27][150/370]\tLoss 5.7560 (5.8330)\tPrec@1 43.750 (39.156)\tPrec@5 54.688 (46.715)\t batch time: 1.18\n",
            "Epoch: [27][180/370]\tLoss 5.9351 (5.8351)\tPrec@1 31.250 (38.946)\tPrec@5 39.062 (46.573)\t batch time: 1.13\n",
            "Epoch: [27][210/370]\tLoss 5.9196 (5.8350)\tPrec@1 32.031 (39.007)\tPrec@5 41.406 (46.560)\t batch time: 1.25\n",
            "Epoch: [27][240/370]\tLoss 5.8808 (5.8352)\tPrec@1 35.938 (39.011)\tPrec@5 43.750 (46.499)\t batch time: 1.16\n",
            "Epoch: [27][270/370]\tLoss 5.9221 (5.8373)\tPrec@1 31.250 (38.832)\tPrec@5 42.969 (46.356)\t batch time: 1.26\n",
            "Epoch: [27][300/370]\tLoss 5.7738 (5.8346)\tPrec@1 44.531 (39.042)\tPrec@5 49.219 (46.647)\t batch time: 1.20\n",
            "Epoch: [27][330/370]\tLoss 5.8286 (5.8339)\tPrec@1 40.625 (39.072)\tPrec@5 50.000 (46.653)\t batch time: 1.19\n",
            "Epoch: [27][360/370]\tLoss 5.9008 (5.8353)\tPrec@1 33.594 (38.980)\tPrec@5 39.844 (46.566)\t batch time: 1.20\n",
            "Test\t  Prec@1: 39.638\t Prec@5: 47.124 (Err: 60.362 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [28][0/370]\tLoss 5.9269 (5.9269)\tPrec@1 32.031 (32.031)\tPrec@5 42.969 (42.969)\t batch time: 4.81\n",
            "Epoch: [28][30/370]\tLoss 5.8113 (5.8254)\tPrec@1 39.844 (39.491)\tPrec@5 46.875 (48.034)\t batch time: 1.15\n",
            "Epoch: [28][60/370]\tLoss 5.8490 (5.8268)\tPrec@1 39.062 (39.383)\tPrec@5 44.531 (47.400)\t batch time: 1.21\n",
            "Epoch: [28][90/370]\tLoss 5.8688 (5.8301)\tPrec@1 36.719 (39.311)\tPrec@5 42.969 (47.107)\t batch time: 1.19\n",
            "Epoch: [28][120/370]\tLoss 5.8931 (5.8332)\tPrec@1 32.812 (39.056)\tPrec@5 39.062 (46.662)\t batch time: 1.25\n",
            "Epoch: [28][150/370]\tLoss 5.8588 (5.8320)\tPrec@1 38.281 (39.192)\tPrec@5 41.406 (46.658)\t batch time: 1.15\n",
            "Epoch: [28][180/370]\tLoss 5.7994 (5.8324)\tPrec@1 41.406 (39.201)\tPrec@5 47.656 (46.499)\t batch time: 1.19\n",
            "Epoch: [28][210/370]\tLoss 5.8007 (5.8325)\tPrec@1 41.406 (39.185)\tPrec@5 49.219 (46.582)\t batch time: 1.19\n",
            "Epoch: [28][240/370]\tLoss 5.8252 (5.8317)\tPrec@1 41.406 (39.228)\tPrec@5 46.875 (46.570)\t batch time: 1.18\n",
            "Epoch: [28][270/370]\tLoss 5.9255 (5.8321)\tPrec@1 31.250 (39.195)\tPrec@5 49.219 (46.702)\t batch time: 1.13\n",
            "Epoch: [28][300/370]\tLoss 5.7467 (5.8322)\tPrec@1 48.438 (39.200)\tPrec@5 54.688 (46.675)\t batch time: 1.27\n",
            "Epoch: [28][330/370]\tLoss 5.8071 (5.8322)\tPrec@1 40.625 (39.176)\tPrec@5 48.438 (46.604)\t batch time: 1.22\n",
            "Epoch: [28][360/370]\tLoss 5.7860 (5.8326)\tPrec@1 41.406 (39.136)\tPrec@5 50.000 (46.596)\t batch time: 1.15\n",
            "Test\t  Prec@1: 39.790\t Prec@5: 46.724 (Err: 60.210 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-04\n",
            "Epoch: [29][0/370]\tLoss 5.7440 (5.7440)\tPrec@1 42.969 (42.969)\tPrec@5 53.125 (53.125)\t batch time: 4.47\n",
            "Epoch: [29][30/370]\tLoss 5.7471 (5.8220)\tPrec@1 45.312 (40.272)\tPrec@5 60.156 (46.951)\t batch time: 1.17\n",
            "Epoch: [29][60/370]\tLoss 5.8658 (5.8374)\tPrec@1 38.281 (39.139)\tPrec@5 46.875 (46.286)\t batch time: 1.26\n",
            "Epoch: [29][90/370]\tLoss 5.7603 (5.8310)\tPrec@1 46.094 (39.475)\tPrec@5 50.781 (46.326)\t batch time: 1.22\n",
            "Epoch: [29][120/370]\tLoss 5.8511 (5.8276)\tPrec@1 36.719 (39.611)\tPrec@5 44.531 (46.617)\t batch time: 1.19\n",
            "Epoch: [29][150/370]\tLoss 5.8629 (5.8292)\tPrec@1 37.500 (39.476)\tPrec@5 42.188 (46.446)\t batch time: 1.26\n",
            "Epoch: [29][180/370]\tLoss 5.7900 (5.8282)\tPrec@1 42.969 (39.503)\tPrec@5 46.875 (46.456)\t batch time: 1.28\n",
            "Epoch: [29][210/370]\tLoss 5.8448 (5.8271)\tPrec@1 37.500 (39.644)\tPrec@5 46.094 (46.634)\t batch time: 1.18\n",
            "Epoch: [29][240/370]\tLoss 5.8002 (5.8265)\tPrec@1 43.750 (39.704)\tPrec@5 51.562 (46.765)\t batch time: 1.27\n",
            "Epoch: [29][270/370]\tLoss 5.8512 (5.8278)\tPrec@1 39.844 (39.633)\tPrec@5 48.438 (46.679)\t batch time: 1.22\n",
            "Epoch: [29][300/370]\tLoss 5.7670 (5.8266)\tPrec@1 47.656 (39.688)\tPrec@5 50.781 (46.750)\t batch time: 1.17\n",
            "Epoch: [29][330/370]\tLoss 5.8030 (5.8286)\tPrec@1 42.969 (39.509)\tPrec@5 50.000 (46.615)\t batch time: 1.12\n",
            "Epoch: [29][360/370]\tLoss 5.8240 (5.8285)\tPrec@1 38.281 (39.534)\tPrec@5 49.219 (46.613)\t batch time: 1.21\n",
            "Test\t  Prec@1: 40.762\t Prec@5: 47.543 (Err: 59.238 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-04\n",
            "Epoch: [30][0/370]\tLoss 5.8547 (5.8547)\tPrec@1 37.500 (37.500)\tPrec@5 42.188 (42.188)\t batch time: 4.46\n",
            "Epoch: [30][30/370]\tLoss 5.8148 (5.8296)\tPrec@1 40.625 (39.743)\tPrec@5 48.438 (46.547)\t batch time: 1.23\n",
            "Epoch: [30][60/370]\tLoss 5.8134 (5.8309)\tPrec@1 42.969 (39.485)\tPrec@5 48.438 (46.427)\t batch time: 1.18\n",
            "Epoch: [30][90/370]\tLoss 5.8153 (5.8339)\tPrec@1 42.969 (39.260)\tPrec@5 50.781 (46.068)\t batch time: 1.18\n",
            "Epoch: [30][120/370]\tLoss 5.8042 (5.8320)\tPrec@1 41.406 (39.334)\tPrec@5 46.094 (46.275)\t batch time: 1.21\n",
            "Epoch: [30][150/370]\tLoss 5.8139 (5.8278)\tPrec@1 37.500 (39.611)\tPrec@5 44.531 (46.575)\t batch time: 1.23\n",
            "Epoch: [30][180/370]\tLoss 5.8079 (5.8292)\tPrec@1 39.844 (39.473)\tPrec@5 48.438 (46.573)\t batch time: 1.16\n",
            "Epoch: [30][210/370]\tLoss 5.8052 (5.8268)\tPrec@1 43.750 (39.707)\tPrec@5 51.562 (46.842)\t batch time: 1.28\n",
            "Epoch: [30][240/370]\tLoss 5.8593 (5.8272)\tPrec@1 38.281 (39.721)\tPrec@5 44.531 (46.833)\t batch time: 1.14\n",
            "Epoch: [30][270/370]\tLoss 5.8094 (5.8270)\tPrec@1 39.844 (39.665)\tPrec@5 45.312 (46.786)\t batch time: 1.21\n",
            "Epoch: [30][300/370]\tLoss 5.7912 (5.8258)\tPrec@1 42.969 (39.706)\tPrec@5 50.781 (46.815)\t batch time: 1.26\n",
            "Epoch: [30][330/370]\tLoss 5.8272 (5.8261)\tPrec@1 40.625 (39.709)\tPrec@5 49.219 (46.792)\t batch time: 1.27\n",
            "Epoch: [30][360/370]\tLoss 5.8440 (5.8270)\tPrec@1 38.281 (39.640)\tPrec@5 43.750 (46.711)\t batch time: 1.25\n",
            "Test\t  Prec@1: 40.895\t Prec@5: 47.429 (Err: 59.105 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-04\n",
            "Epoch: [31][0/370]\tLoss 5.8766 (5.8766)\tPrec@1 35.156 (35.156)\tPrec@5 40.625 (40.625)\t batch time: 4.68\n",
            "Epoch: [31][30/370]\tLoss 5.8014 (5.8278)\tPrec@1 40.625 (39.617)\tPrec@5 50.000 (46.673)\t batch time: 1.15\n",
            "Epoch: [31][60/370]\tLoss 5.8050 (5.8291)\tPrec@1 39.844 (39.498)\tPrec@5 46.875 (46.376)\t batch time: 1.32\n",
            "Epoch: [31][90/370]\tLoss 5.7616 (5.8255)\tPrec@1 42.969 (39.758)\tPrec@5 50.781 (46.532)\t batch time: 1.22\n",
            "Epoch: [31][120/370]\tLoss 5.9071 (5.8249)\tPrec@1 32.812 (39.902)\tPrec@5 42.969 (46.668)\t batch time: 1.25\n",
            "Epoch: [31][150/370]\tLoss 5.8473 (5.8263)\tPrec@1 39.062 (39.725)\tPrec@5 39.062 (46.590)\t batch time: 1.24\n",
            "Epoch: [31][180/370]\tLoss 5.8233 (5.8263)\tPrec@1 39.844 (39.671)\tPrec@5 50.781 (46.478)\t batch time: 1.30\n",
            "Epoch: [31][210/370]\tLoss 5.9172 (5.8269)\tPrec@1 32.031 (39.670)\tPrec@5 39.844 (46.542)\t batch time: 1.22\n",
            "Epoch: [31][240/370]\tLoss 5.7966 (5.8269)\tPrec@1 43.750 (39.652)\tPrec@5 49.219 (46.629)\t batch time: 1.26\n",
            "Epoch: [31][270/370]\tLoss 5.8167 (5.8280)\tPrec@1 39.844 (39.530)\tPrec@5 47.656 (46.613)\t batch time: 1.33\n",
            "Epoch: [31][300/370]\tLoss 5.7630 (5.8267)\tPrec@1 42.969 (39.659)\tPrec@5 49.219 (46.709)\t batch time: 1.22\n",
            "Epoch: [31][330/370]\tLoss 5.9158 (5.8270)\tPrec@1 32.812 (39.605)\tPrec@5 42.188 (46.639)\t batch time: 1.17\n",
            "Epoch: [31][360/370]\tLoss 5.8731 (5.8267)\tPrec@1 35.938 (39.647)\tPrec@5 42.969 (46.704)\t batch time: 1.19\n",
            "Test\t  Prec@1: 40.933\t Prec@5: 47.352 (Err: 59.067 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-04\n",
            "Epoch: [32][0/370]\tLoss 5.8915 (5.8915)\tPrec@1 32.812 (32.812)\tPrec@5 39.844 (39.844)\t batch time: 4.63\n",
            "Epoch: [32][30/370]\tLoss 5.9324 (5.8339)\tPrec@1 29.688 (39.289)\tPrec@5 36.719 (45.917)\t batch time: 1.33\n",
            "Epoch: [32][60/370]\tLoss 5.8730 (5.8324)\tPrec@1 37.500 (39.152)\tPrec@5 45.312 (45.953)\t batch time: 1.27\n",
            "Epoch: [32][90/370]\tLoss 5.8435 (5.8243)\tPrec@1 37.500 (39.775)\tPrec@5 46.094 (46.523)\t batch time: 1.19\n",
            "Epoch: [32][120/370]\tLoss 5.8187 (5.8272)\tPrec@1 41.406 (39.689)\tPrec@5 45.312 (46.417)\t batch time: 1.15\n",
            "Epoch: [32][150/370]\tLoss 5.8505 (5.8266)\tPrec@1 38.281 (39.730)\tPrec@5 43.750 (46.461)\t batch time: 1.16\n",
            "Epoch: [32][180/370]\tLoss 5.8343 (5.8245)\tPrec@1 41.406 (39.896)\tPrec@5 46.875 (46.607)\t batch time: 1.15\n",
            "Epoch: [32][210/370]\tLoss 5.8504 (5.8239)\tPrec@1 37.500 (39.922)\tPrec@5 44.531 (46.623)\t batch time: 1.24\n",
            "Epoch: [32][240/370]\tLoss 5.8229 (5.8247)\tPrec@1 39.062 (39.892)\tPrec@5 46.094 (46.583)\t batch time: 1.27\n",
            "Epoch: [32][270/370]\tLoss 5.7770 (5.8258)\tPrec@1 43.750 (39.821)\tPrec@5 50.000 (46.569)\t batch time: 1.20\n",
            "Epoch: [32][300/370]\tLoss 5.8005 (5.8241)\tPrec@1 42.188 (39.974)\tPrec@5 49.219 (46.680)\t batch time: 1.16\n",
            "Epoch: [32][330/370]\tLoss 5.8476 (5.8242)\tPrec@1 37.500 (39.957)\tPrec@5 46.094 (46.674)\t batch time: 1.20\n",
            "Epoch: [32][360/370]\tLoss 5.8289 (5.8242)\tPrec@1 37.500 (39.926)\tPrec@5 45.312 (46.635)\t batch time: 1.27\n",
            "Test\t  Prec@1: 40.876\t Prec@5: 47.257 (Err: 59.124 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-04\n",
            "Epoch: [33][0/370]\tLoss 5.8348 (5.8348)\tPrec@1 38.281 (38.281)\tPrec@5 45.312 (45.312)\t batch time: 4.46\n",
            "Epoch: [33][30/370]\tLoss 5.7663 (5.8184)\tPrec@1 43.750 (40.272)\tPrec@5 48.438 (46.522)\t batch time: 1.20\n",
            "Epoch: [33][60/370]\tLoss 5.7716 (5.8169)\tPrec@1 42.969 (40.446)\tPrec@5 52.344 (47.067)\t batch time: 1.31\n",
            "Epoch: [33][90/370]\tLoss 5.8075 (5.8155)\tPrec@1 39.844 (40.634)\tPrec@5 46.875 (47.218)\t batch time: 1.14\n",
            "Epoch: [33][120/370]\tLoss 5.7917 (5.8179)\tPrec@1 42.969 (40.347)\tPrec@5 50.781 (47.178)\t batch time: 1.20\n",
            "Epoch: [33][150/370]\tLoss 5.8131 (5.8163)\tPrec@1 38.281 (40.459)\tPrec@5 44.531 (47.284)\t batch time: 1.18\n",
            "Epoch: [33][180/370]\tLoss 5.8136 (5.8203)\tPrec@1 39.844 (40.142)\tPrec@5 46.875 (47.048)\t batch time: 1.42\n",
            "Epoch: [33][210/370]\tLoss 5.8846 (5.8215)\tPrec@1 35.938 (40.018)\tPrec@5 42.969 (46.971)\t batch time: 1.19\n",
            "Epoch: [33][240/370]\tLoss 5.8844 (5.8215)\tPrec@1 36.719 (40.025)\tPrec@5 43.750 (46.979)\t batch time: 1.18\n",
            "Epoch: [33][270/370]\tLoss 5.8053 (5.8228)\tPrec@1 42.188 (39.956)\tPrec@5 49.219 (46.832)\t batch time: 1.24\n",
            "Epoch: [33][300/370]\tLoss 5.8308 (5.8226)\tPrec@1 37.500 (39.966)\tPrec@5 48.438 (46.878)\t batch time: 1.19\n",
            "Epoch: [33][330/370]\tLoss 5.7947 (5.8225)\tPrec@1 42.188 (39.995)\tPrec@5 46.094 (46.877)\t batch time: 1.16\n",
            "Epoch: [33][360/370]\tLoss 5.8624 (5.8233)\tPrec@1 35.156 (39.926)\tPrec@5 43.750 (46.812)\t batch time: 1.15\n",
            "Test\t  Prec@1: 40.838\t Prec@5: 47.257 (Err: 59.162 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-04\n",
            "Epoch: [34][0/370]\tLoss 5.7199 (5.7199)\tPrec@1 47.656 (47.656)\tPrec@5 53.125 (53.125)\t batch time: 4.09\n",
            "Epoch: [34][30/370]\tLoss 5.8042 (5.8131)\tPrec@1 42.969 (41.053)\tPrec@5 46.094 (47.681)\t batch time: 1.15\n",
            "Epoch: [34][60/370]\tLoss 5.8064 (5.8244)\tPrec@1 40.625 (40.036)\tPrec@5 47.656 (46.542)\t batch time: 1.29\n",
            "Epoch: [34][90/370]\tLoss 5.7816 (5.8213)\tPrec@1 43.750 (40.264)\tPrec@5 50.000 (46.849)\t batch time: 1.16\n",
            "Epoch: [34][120/370]\tLoss 5.8556 (5.8218)\tPrec@1 38.281 (40.167)\tPrec@5 49.219 (46.869)\t batch time: 1.25\n",
            "Epoch: [34][150/370]\tLoss 5.8014 (5.8238)\tPrec@1 42.188 (40.040)\tPrec@5 48.438 (46.922)\t batch time: 1.28\n",
            "Epoch: [34][180/370]\tLoss 5.8734 (5.8232)\tPrec@1 34.375 (40.064)\tPrec@5 41.406 (46.979)\t batch time: 1.22\n",
            "Epoch: [34][210/370]\tLoss 5.7483 (5.8234)\tPrec@1 46.875 (40.047)\tPrec@5 51.562 (46.882)\t batch time: 1.22\n",
            "Epoch: [34][240/370]\tLoss 5.8037 (5.8236)\tPrec@1 41.406 (39.990)\tPrec@5 50.000 (46.787)\t batch time: 1.17\n",
            "Epoch: [34][270/370]\tLoss 5.7404 (5.8243)\tPrec@1 47.656 (39.945)\tPrec@5 55.469 (46.734)\t batch time: 1.32\n",
            "Epoch: [34][300/370]\tLoss 5.8322 (5.8237)\tPrec@1 38.281 (39.966)\tPrec@5 44.531 (46.748)\t batch time: 1.19\n",
            "Epoch: [34][330/370]\tLoss 5.8606 (5.8245)\tPrec@1 35.938 (39.910)\tPrec@5 42.188 (46.691)\t batch time: 1.26\n",
            "Epoch: [34][360/370]\tLoss 5.8165 (5.8242)\tPrec@1 42.188 (39.937)\tPrec@5 49.219 (46.702)\t batch time: 1.24\n",
            "Test\t  Prec@1: 40.800\t Prec@5: 47.429 (Err: 59.200 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-04\n",
            "Epoch: [35][0/370]\tLoss 5.8513 (5.8513)\tPrec@1 38.281 (38.281)\tPrec@5 42.969 (42.969)\t batch time: 5.11\n",
            "Epoch: [35][30/370]\tLoss 5.8528 (5.8213)\tPrec@1 36.719 (40.045)\tPrec@5 46.094 (46.699)\t batch time: 1.19\n",
            "Epoch: [35][60/370]\tLoss 5.8259 (5.8239)\tPrec@1 39.062 (39.869)\tPrec@5 47.656 (46.542)\t batch time: 1.13\n",
            "Epoch: [35][90/370]\tLoss 5.7627 (5.8213)\tPrec@1 44.531 (40.187)\tPrec@5 52.344 (47.012)\t batch time: 1.15\n",
            "Epoch: [35][120/370]\tLoss 5.8532 (5.8223)\tPrec@1 37.500 (40.005)\tPrec@5 45.312 (46.752)\t batch time: 1.24\n",
            "Epoch: [35][150/370]\tLoss 5.8713 (5.8224)\tPrec@1 38.281 (39.999)\tPrec@5 45.312 (46.828)\t batch time: 1.24\n",
            "Epoch: [35][180/370]\tLoss 5.8448 (5.8208)\tPrec@1 36.719 (40.124)\tPrec@5 40.625 (47.022)\t batch time: 1.24\n",
            "Epoch: [35][210/370]\tLoss 5.7461 (5.8206)\tPrec@1 47.656 (40.114)\tPrec@5 52.344 (46.934)\t batch time: 1.19\n",
            "Epoch: [35][240/370]\tLoss 5.8887 (5.8233)\tPrec@1 35.938 (39.870)\tPrec@5 40.625 (46.642)\t batch time: 1.31\n",
            "Epoch: [35][270/370]\tLoss 5.8136 (5.8222)\tPrec@1 42.188 (39.953)\tPrec@5 47.656 (46.699)\t batch time: 1.19\n",
            "Epoch: [35][300/370]\tLoss 5.8153 (5.8240)\tPrec@1 39.844 (39.849)\tPrec@5 50.781 (46.615)\t batch time: 1.13\n",
            "Epoch: [35][330/370]\tLoss 5.8001 (5.8234)\tPrec@1 42.188 (39.915)\tPrec@5 48.438 (46.660)\t batch time: 1.27\n",
            "Epoch: [35][360/370]\tLoss 5.8830 (5.8235)\tPrec@1 33.594 (39.930)\tPrec@5 39.062 (46.708)\t batch time: 1.25\n",
            "Test\t  Prec@1: 40.876\t Prec@5: 47.352 (Err: 59.124 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-04\n",
            "Epoch: [36][0/370]\tLoss 5.7708 (5.7708)\tPrec@1 45.312 (45.312)\tPrec@5 53.906 (53.906)\t batch time: 4.30\n",
            "Epoch: [36][30/370]\tLoss 5.7838 (5.8333)\tPrec@1 42.188 (38.886)\tPrec@5 46.094 (45.035)\t batch time: 1.26\n",
            "Epoch: [36][60/370]\tLoss 5.8490 (5.8334)\tPrec@1 37.500 (39.011)\tPrec@5 44.531 (45.300)\t batch time: 1.28\n",
            "Epoch: [36][90/370]\tLoss 5.8537 (5.8360)\tPrec@1 36.719 (38.994)\tPrec@5 44.531 (45.493)\t batch time: 1.24\n",
            "Epoch: [36][120/370]\tLoss 5.8137 (5.8312)\tPrec@1 41.406 (39.347)\tPrec@5 47.656 (45.790)\t batch time: 1.26\n",
            "Epoch: [36][150/370]\tLoss 5.8445 (5.8309)\tPrec@1 37.500 (39.306)\tPrec@5 47.656 (45.959)\t batch time: 1.19\n",
            "Epoch: [36][180/370]\tLoss 5.8206 (5.8292)\tPrec@1 38.281 (39.460)\tPrec@5 47.656 (46.215)\t batch time: 1.34\n",
            "Epoch: [36][210/370]\tLoss 5.7504 (5.8294)\tPrec@1 45.312 (39.477)\tPrec@5 53.906 (46.246)\t batch time: 1.20\n",
            "Epoch: [36][240/370]\tLoss 5.9366 (5.8296)\tPrec@1 32.031 (39.452)\tPrec@5 38.281 (46.207)\t batch time: 1.20\n",
            "Epoch: [36][270/370]\tLoss 5.8429 (5.8285)\tPrec@1 38.281 (39.581)\tPrec@5 48.438 (46.353)\t batch time: 1.14\n",
            "Epoch: [36][300/370]\tLoss 5.7574 (5.8276)\tPrec@1 47.656 (39.621)\tPrec@5 52.344 (46.452)\t batch time: 1.21\n",
            "Epoch: [36][330/370]\tLoss 5.8983 (5.8274)\tPrec@1 35.938 (39.636)\tPrec@5 42.188 (46.457)\t batch time: 1.17\n",
            "Epoch: [36][360/370]\tLoss 5.8579 (5.8252)\tPrec@1 38.281 (39.807)\tPrec@5 44.531 (46.594)\t batch time: 1.19\n",
            "Test\t  Prec@1: 41.029\t Prec@5: 47.448 (Err: 58.971 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-04\n",
            "Epoch: [37][0/370]\tLoss 5.8241 (5.8241)\tPrec@1 39.844 (39.844)\tPrec@5 42.188 (42.188)\t batch time: 4.52\n",
            "Epoch: [37][30/370]\tLoss 5.8338 (5.8111)\tPrec@1 39.062 (40.827)\tPrec@5 46.875 (47.681)\t batch time: 1.14\n",
            "Epoch: [37][60/370]\tLoss 5.8124 (5.8232)\tPrec@1 41.406 (39.908)\tPrec@5 49.219 (46.849)\t batch time: 1.22\n",
            "Epoch: [37][90/370]\tLoss 5.8415 (5.8276)\tPrec@1 41.406 (39.629)\tPrec@5 48.438 (46.652)\t batch time: 1.22\n",
            "Epoch: [37][120/370]\tLoss 5.9000 (5.8205)\tPrec@1 35.938 (40.160)\tPrec@5 42.188 (47.127)\t batch time: 1.22\n",
            "Epoch: [37][150/370]\tLoss 5.7027 (5.8217)\tPrec@1 49.219 (40.009)\tPrec@5 54.688 (46.916)\t batch time: 1.22\n",
            "Epoch: [37][180/370]\tLoss 5.8285 (5.8231)\tPrec@1 38.281 (39.887)\tPrec@5 42.969 (46.853)\t batch time: 1.17\n",
            "Epoch: [37][210/370]\tLoss 5.8664 (5.8227)\tPrec@1 35.156 (39.899)\tPrec@5 40.625 (46.801)\t batch time: 1.25\n",
            "Epoch: [37][240/370]\tLoss 5.8491 (5.8246)\tPrec@1 36.719 (39.743)\tPrec@5 43.750 (46.690)\t batch time: 1.27\n",
            "Epoch: [37][270/370]\tLoss 5.8284 (5.8239)\tPrec@1 39.062 (39.850)\tPrec@5 46.875 (46.705)\t batch time: 1.19\n",
            "Epoch: [37][300/370]\tLoss 5.8466 (5.8241)\tPrec@1 36.719 (39.836)\tPrec@5 42.188 (46.683)\t batch time: 1.20\n",
            "Epoch: [37][330/370]\tLoss 5.7592 (5.8247)\tPrec@1 42.969 (39.825)\tPrec@5 53.125 (46.660)\t batch time: 1.40\n",
            "Epoch: [37][360/370]\tLoss 5.8200 (5.8236)\tPrec@1 42.188 (39.919)\tPrec@5 46.875 (46.717)\t batch time: 1.17\n",
            "Test\t  Prec@1: 40.895\t Prec@5: 47.371 (Err: 59.105 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-04\n",
            "Epoch: [38][0/370]\tLoss 5.8628 (5.8628)\tPrec@1 37.500 (37.500)\tPrec@5 44.531 (44.531)\t batch time: 4.08\n",
            "Epoch: [38][30/370]\tLoss 5.7793 (5.8327)\tPrec@1 44.531 (39.264)\tPrec@5 52.344 (46.774)\t batch time: 1.18\n",
            "Epoch: [38][60/370]\tLoss 5.7808 (5.8290)\tPrec@1 43.750 (39.485)\tPrec@5 50.781 (46.683)\t batch time: 1.23\n",
            "Epoch: [38][90/370]\tLoss 5.7527 (5.8287)\tPrec@1 44.531 (39.432)\tPrec@5 50.000 (46.480)\t batch time: 1.21\n",
            "Epoch: [38][120/370]\tLoss 5.7806 (5.8216)\tPrec@1 45.312 (40.063)\tPrec@5 52.344 (47.166)\t batch time: 1.24\n",
            "Epoch: [38][150/370]\tLoss 5.7943 (5.8257)\tPrec@1 41.406 (39.797)\tPrec@5 49.219 (46.834)\t batch time: 1.19\n",
            "Epoch: [38][180/370]\tLoss 5.7909 (5.8242)\tPrec@1 43.750 (39.870)\tPrec@5 48.438 (46.866)\t batch time: 1.19\n",
            "Epoch: [38][210/370]\tLoss 5.7909 (5.8224)\tPrec@1 44.531 (40.007)\tPrec@5 45.312 (46.919)\t batch time: 1.29\n",
            "Epoch: [38][240/370]\tLoss 5.8171 (5.8228)\tPrec@1 39.844 (40.016)\tPrec@5 48.438 (46.885)\t batch time: 1.21\n",
            "Epoch: [38][270/370]\tLoss 5.7476 (5.8234)\tPrec@1 46.094 (39.919)\tPrec@5 52.344 (46.869)\t batch time: 1.24\n",
            "Epoch: [38][300/370]\tLoss 5.8651 (5.8239)\tPrec@1 35.156 (39.865)\tPrec@5 42.188 (46.808)\t batch time: 1.19\n",
            "Epoch: [38][330/370]\tLoss 5.7924 (5.8233)\tPrec@1 43.750 (39.891)\tPrec@5 47.656 (46.828)\t batch time: 1.17\n",
            "Epoch: [38][360/370]\tLoss 5.8276 (5.8244)\tPrec@1 38.281 (39.809)\tPrec@5 46.094 (46.808)\t batch time: 1.20\n",
            "Test\t  Prec@1: 41.029\t Prec@5: 47.352 (Err: 58.971 )\n",
            "\n",
            "Training googlenet model\n",
            "current lr 1.00000e-04\n",
            "Epoch: [39][0/370]\tLoss 5.9101 (5.9101)\tPrec@1 31.250 (31.250)\tPrec@5 40.625 (40.625)\t batch time: 4.50\n",
            "Epoch: [39][30/370]\tLoss 5.8157 (5.8142)\tPrec@1 39.062 (40.398)\tPrec@5 42.188 (46.951)\t batch time: 1.21\n",
            "Epoch: [39][60/370]\tLoss 5.8853 (5.8239)\tPrec@1 38.281 (40.138)\tPrec@5 46.875 (46.837)\t batch time: 1.30\n",
            "Epoch: [39][90/370]\tLoss 5.8186 (5.8274)\tPrec@1 38.281 (39.749)\tPrec@5 44.531 (46.437)\t batch time: 1.26\n",
            "Epoch: [39][120/370]\tLoss 5.8509 (5.8245)\tPrec@1 37.500 (39.779)\tPrec@5 43.750 (46.494)\t batch time: 1.11\n",
            "Epoch: [39][150/370]\tLoss 5.8247 (5.8232)\tPrec@1 39.844 (39.927)\tPrec@5 44.531 (46.642)\t batch time: 1.22\n",
            "Epoch: [39][180/370]\tLoss 5.8821 (5.8230)\tPrec@1 37.500 (39.969)\tPrec@5 43.750 (46.694)\t batch time: 1.27\n",
            "Epoch: [39][210/370]\tLoss 5.8074 (5.8224)\tPrec@1 42.969 (40.029)\tPrec@5 50.000 (46.764)\t batch time: 1.24\n",
            "Epoch: [39][240/370]\tLoss 5.8326 (5.8228)\tPrec@1 38.281 (40.035)\tPrec@5 46.094 (46.869)\t batch time: 1.23\n",
            "Epoch: [39][270/370]\tLoss 5.8167 (5.8240)\tPrec@1 39.844 (39.962)\tPrec@5 49.219 (46.789)\t batch time: 1.23\n",
            "Epoch: [39][300/370]\tLoss 5.8185 (5.8235)\tPrec@1 39.062 (39.997)\tPrec@5 46.875 (46.836)\t batch time: 1.18\n",
            "Epoch: [39][330/370]\tLoss 5.8157 (5.8229)\tPrec@1 39.062 (40.044)\tPrec@5 43.750 (46.920)\t batch time: 1.28\n",
            "Epoch: [39][360/370]\tLoss 5.7473 (5.8230)\tPrec@1 46.094 (40.047)\tPrec@5 53.906 (46.873)\t batch time: 1.20\n",
            "Test\t  Prec@1: 41.124\t Prec@5: 47.467 (Err: 58.876 )\n",
            "\n",
            "The lowest error from alexnet model after 55 epochs is 58.876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkpS_XxO_cPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path1 = '/content/gdrive/My Drive/Hide and Seek/data/test'\n",
        "# x = os.listdir(path1)\n",
        "# for element in x:\n",
        "#   print(element)\n",
        "#   images = os.listdir(os.path.join(path, element))\n",
        "#   for i in range(len(images)):\n",
        "#     if i < 75:\n",
        "#       pass\n",
        "#     else:\n",
        "#       os.remove(os.path.join(os.path.join(path, element), images[i]))\n",
        "\n",
        "# for element in x:\n",
        "#   print(element)\n",
        "#   images = os.listdir(os.path.join(path1, element))\n",
        "#   print(len(images))\n",
        "# hfhfhuhjg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l13gC6rABg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}